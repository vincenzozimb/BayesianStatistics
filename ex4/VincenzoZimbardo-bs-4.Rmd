---
title: Homework 4 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
---

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

# Exercise 1: Mixture of binomials

```{r eval=TRUE, echo=FALSE}
set.seed(1236)    
theta <- runif(4)
m <- rpois(40, lambda=30)
p <- sample(theta, size=40, replace=TRUE)
y <- rbinom(40, size=m, prob=p)      
```
In the below table there are $40$ observations $y_i \sim Bin(m_i, p_i)$.

```{r eval=TRUE, echo=FALSE}
library(knitr)
x <- cbind(m,y)
colnames(x) <- c("m", "y")
kable(t(x))
```

### Assume that the number of distinct $p_i$s is $4$. Implement in R a Gibbs sampler based on data augmentation

Let $n$ be the number of observations, $k$ the number of classes. The index $i$ runs in $1, \dots, n$ while the index $j$ in $1, \dots, k$.

For every $i$, let $q_i \in {1, \dots, k}$ and $q_i=j$ iff data $y_i$ belongs to class $j$. 
The data augmentation matrix has components $z_{ij}=1$ iff $q_i=j$, and $z_{ij}=0$ otherwise. 

The complete likelihood is:

\begin{align}
    f(y | \alpha, p, z) &= \prod_{i=1}^{n} \binom{m_i}{y_i} p_{q_i}^{y_i}(1-p_{q_i})^{m_i-y_i} \\
    &= \prod_{i=1}^{n} \binom{m_i}{y_i} \prod_{j=1}^{k} [p_j^{y_i}(1-p_j)^{m_i-y_i}]^{z_{ij}} 
\end{align}

Prior on $z$. The row of $z$ are independent and constructed assuming a categorical distribution as a model

Prior on $\alpha$. As the alphas form a discrete probability distribution, the vector $\alpha$ must belong to the simplex of order $k-1$. 
Thus we choose as a prior distribution a Dirichlet distribution:

\begin{equation}
    f(\alpha) \propto \prod_{j=1}^{k} \alpha_j^{c_j-1}
\end{equation}

Prior on $p$. We assume each $p$ to be independent and distributed according to a beta distribution:

\begin{equation}
    f(p) \propto \prod_{j=1}^{k} p_j^{a_j-1}(1-p_j)^{b_j-1}
\end{equation}

Applying Bayes' theorem, the posterior distribution is:

\begin{equation}
    f(\alpha, p, z | y) \propto f(y | \alpha, p, z) \mathbb{P}(z | \alpha) f(\alpha) f(p)
\end{equation}

In order to implement a Gibbs sampler, we need the full conditionals. Exploiting conjugacy we obtain:

\begin{align}
    \alpha | p, z, y &\sim \mathcal{Dir}(c_1+z_{+1}, \dots, c_k+z_{+k}) \\
    p_j | \alpha, z, y, &\sim \mathcal{Beta}(a_j+y_{+j}, b_j+m_{+j}-y_{+j}) \\
    \mathbb{P}(z_{ij}=1 | \alpha, p, y) &\propto \alpha_j p_j^{y_i} (1-p_j)^{m_i-y_i} =: r_{ij} 
\end{align}

where $z_{+j}:=\sum_{i=1}^{n}z_{ij}$, $y_{+j}:=\sum_{i=1}^{n}z_{ij}y_i$ and $m_{+j}:=\sum_{i=1}^{n}z_{ij}m_i$.
The meaning of the last one is $q_i | \alpha, p, y \sim \mathcal{Cat}(r_{i1}, \dots, r_{ik})$.

```{r echo=TRUE}
k <- 4
n <- length(y)  
# hyper-parameters for the beta distribution
a <- rep(0.5,k)
b <- rep(0.5,k)
# hyper-parameters for the Dirichlet distribution
c <- rep(1,k)
# initial parameters (randomly generated)
alpha_0 <- runif(k)
alpha_0 <- alpha_0 / sum(alpha_0)
p_0 <- sort(runif(k))
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

```{r echo=TRUE}
library(DirichletReg)
# sampling from full conditional for alpha
sample_alpha <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    z_plus <- colSums(z)
    c_new <- c + z_plus
    # return
    rdirichlet(1, c_new)
}

# sampling from full conditional for p
sample_p <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    y_plus <- y %*% z
    m_plus <- m %*% z
    a_new <- a + y_plus
    b_new <- b + m_plus - y_plus    
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rbeta(1, a_new[j], b_new[j]) 
    }
    # return
    return (samples)    
}

# sampling from full conditional for z
sample_z <- function(alpha, p){
    # input control
    stopifnot(length(alpha)==k)
    stopifnot(length(p)==k)
    # function
    r <- matrix(nrow=n, ncol=k)
    for (i in 1:n) {
       for (j in 1:k) {
          r[i,j] <- alpha[j] * p[j]^y[i] * (1-p[j])^(m[i]-y[i])
       }
    }
    f <- rep(0,n)
    for (i in 1:n) {
       f[i] <- sample(1:k, size=1, prob=r[i,])
    }
    z <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       z[i,f[i]] <- 1
    }
    # return
    return (z)    
}
```

```{r echo=TRUE}
# Gibbs sampler
gibbs <- function(n_samples, alpha_0, p_0, z_0){

    # input control
    stopifnot(nrow(z_0)==n)
    stopifnot(ncol(z_0)==k)
    stopifnot(length(alpha_0)==k)
    stopifnot(length(p_0)==k)

    # initial values
    alpha_curr <- alpha_0
    p_curr <- p_0
    z_curr <- z_0
  
    # parameters to be monitored
    alpha_samples <- matrix(nrow = n_samples, ncol = k)
    p_samples <- matrix(nrow = n_samples, ncol = k)

    # iteration
    for (i in 1:n_samples) {
        alpha_curr <- sample_alpha(z_curr)
        p_curr <- sample_p(z_curr)
        z_curr <- sample_z(alpha_curr, p_curr)
        # label switching
        order <- order(p_curr)
        p_curr <- p_curr[order]
        alpha_curr <- alpha_curr[order]
        z_curr <- z_curr[,order]
        # saving
        alpha_samples[i,] <- alpha_curr
        p_samples[i, ] <- p_curr
    }

    # return
    return (list(alpha_samples=alpha_samples, p_samples=p_samples))    
}
```

### Run the Gibbs sampler for the above data and summurize your finding

Perform sampling

```{r echo=TRUE}
n_samples <- 1000
burn_in <- 100
samples <- gibbs(n_samples+burn_in, alpha_0, p_0, z_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("alpha[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("p[", j, "]"))
}
samples <- cbind(samples$alpha_samples, samples$p_samples)
samples <- samples[-(1:burn_in),]
colnames(samples) <- names
```

MCMC plots 

```{r echo=FALSE, eval=TRUE}
# trace  to check convergence 
par(mfrow = c(2, k))
for (j in 1:k) {
    plot(1:n_samples,samples[,j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[j])
}
for (j in 1:k) {
    plot(1:n_samples,samples[,k+j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[k+j])   
}
# Restore default plot options
par(mfrow = c(1, 1))
# correlation
# acf(samples)
```

Histograms

```{r echo=FALSE}
par(mfrow = c(2, k))
for (j in 1:k) {
    hist(samples[,j], main=colnames(samples)[j], xlab=NULL)
}
for (j in 1:k) {
    hist(samples[,k+j], main=colnames(samples)[k+j], xlab=NULL)
}
# Restore default plot options
par(mfrow = c(1, 1))
```

Summary

```{r echo=TRUE}
means <- colMeans(samples)
sds <- apply(samples, 2, sd)
summ <- rbind(means, sds)
colnames(summ) <- names
rownames(summ) <- c("mean", "sd")
kable(summ)
```



### Use JAGS to implement the same model and compare the results  

```{r echo=TRUE}
data_list <- list(
  "y" = y,
  "m" = m,
  "n" = n,
  "k" = k,
  "a" = a,
  "b" = b,
  "c" = c
)
str(data_list)
```

```{r echo=TRUE}
library(rjags)
library(R2jags)

# Define the model
binomial_mixture <- "model {
    # prior on alpha
    alpha ~ ddirch(c[])
    
    # prior on q
    for (i in 1:n){
        q[i] ~ dcat(alpha[])
    }

    # prior on p
    for (j in 1:k) {
        p[j] ~ dbeta(a[j], b[j])
    }

    # Likelihood
    for (i in 1:n) { 
        y[i] ~ dbin(p[q[i]], m[i])
    }

}"

# Specify desired parameters for inference
parameters <- c("alpha", "p")

# Run
n_samples <- 1000
n.burnin <- 0
n.thin   <- 1

MCMCrun <- jags(
  data = data_list,
  parameters.to.save = parameters,
  model.file = textConnection(binomial_mixture),
  n.burnin = n.burnin,
  n.iter = n.thin*n_samples + n.burnin,
  n.chains = 1,
  n.thin = n.thin,
  progress.bar = "none",
  quiet = TRUE,
  DIC = FALSE
)
```

```{r echo=TRUE}
# Posterior samples extraction
samples_list <- MCMCrun$BUGSoutput$sims.list
str(samples_list)
# Convert list to matrix
samples <- cbind(samples_list$alpha, samples_list$p)
names <- c()
for (i in 1:k) {
   names <- c(names, paste0("alpha[", i, "]"))
}
for (i in 1:k) {
   names <- c(names, paste0("p[", i, "]"))
}
colnames(samples) <- names
# Define the maximum number of rows to display
max_row <- 4
# Display a subset of rows and columns from the large matrix
subset_matrix <- samples[1:max_row, ]
round(subset_matrix, 4)
```

MCMC plots 

```{r echo=FALSE, eval=TRUE}
# trace  to check convergence 
par(mfrow = c(2, k))
for (j in 1:k) {
    plot(1:n_samples,samples[,j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[j])
}
for (j in 1:k) {
    plot(1:n_samples,samples[,k+j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[k+j])   
}
# Restore default plot options
par(mfrow = c(1, 1))
# correlation
# acf(samples)
```

Histograms

```{r echo=FALSE}
par(mfrow = c(2, k))
for (j in 1:k) {
    hist(samples[,j], main=colnames(samples)[j], xlab=NULL)
}
for (j in 1:k) {
    hist(samples[,k+j], main=colnames(samples)[k+j], xlab=NULL)
}
# Restore default plot options
par(mfrow = c(1, 1))
```

Summary

```{r echo=TRUE}
summ <- MCMCrun$BUGSoutput$summary
summ <- round(summ[,c(1,2)],4)
kable(t(summ))
```


#### Comparison

The results obtained with JAGS are expected to be more reliable. 
The differences in the results are due to the correlation stucture of the sample generated with the implemented version of the Gibbs sampler.
It can be notice also from the trace plots that the implemented version of the Gibbs sampler explore the configuration space in a weaker way:
it seems that $2-3$ weakly connected regions are present, leading indeed to multimodal histograms for $p$.
JAGS is able to efficienlty sample also the intermediate (of lower probability) regions. 


### Assume now the number of components (distinct $p_i$) is unknown. Implement in R a non parametric Gibbs sampler with stick-breaking approach and Dirichlet prior

The stick-breaking construction of a Dirichlet process with base measure $H$ and concentration parameter $\alpha$ is as follows:

\begin{align}
    \beta_k &\sim \mathcal{Beta}(1,\alpha) \\
    \pi_k &:= \beta_k \prod_{j=1}^{k-1}(1-\beta_j) \Leftrightarrow \pi \sim GEM(\alpha)  \\
    \theta_k &\sim H
\end{align}

Then the random probability measure defined as $G:=\sum_{k=1}^{\infty}\pi_k \delta_{\theta_k}$ is distributed according to $G \sim DP(\alpha, H)$.


A non-parametric bayesian mixture model is a model in the form:

\begin{align}
    y_i | \theta_i &\sim F_{\theta_i} \\
    \theta_i | G &\sim G \\
    G | \alpha, H &\sim DP(\alpha, H)
\end{align}

Being $G$ a realization of a Dirichlet process, it is a discrete probability distribution. This means that multiple $\theta_i$'s can take the 
same value simultaneously, hence the interpretation of the model as a mixture model.

We can rewrite the model using the stick-breaking construction. Let $z_i$ be a class assigment variable, taking value $j$ with probability $\pi_j$:

\begin{align}
    y_i | z_i, \{ \theta_j \}_j &\sim F_{\theta_{z_i}} \\
    \theta_j &\sim H \\
    z_i | \pi &\sim \mathcal{Cat}(\pi) \\
    \pi &\sim GEM(\alpha)
\end{align}

From a practical point of view, we need to fix a sufficiently large upper bound $k$ and truncate the random measure $G$.
In this way we obtain an approximate representation of the Dirichlet process.
Of course the worst case scenario (complete absence of any prior information) would require at most to fix $k$ to the number of data. 
The algorithm will select, based on the data, only the relevant classes, according to the non parametric paradigm. 

In our model:

\begin{align}
    y_i | q_i, \{ p_j \}_j &\sim \mathcal{Bin}(m_i,p_{q_i}) \\
    p &\sim \mathcal{Dir}(\alpha/k, \dots, \alpha/k) \\
    q_i | \pi &\sim \mathcal{Cat}(\pi_1, \dots, \pi_k) \\
    \pi &\sim GEM(\alpha)
\end{align}

#### Sample from the posterior: Blocked Gibbs sampler.

The state space of the Gibbs sampler is $\{ p=(p_1, \dots, p_k), \pi=(\pi_1, \dots, \pi_k), q_1, \dots, q_n \}$. 
The sampling iteration procedes as follows:

1. Sample $p | q_1, \dots, q_n, y$

2. Sample $q_1, \dots, q_n | p, \pi, y$

3. Sample $\pi | q_1, \dots, q_n$

Where:

1. Exploiting conjugacy, each $p_j$ is sampled from a $\mathcal{Beta}(\alpha/k + y_{+j}, \alpha(k-1)/k + m_{+j} - y_{+j})$

2. For each $i$, calculate $\pi_{ij} \propto \pi_j \binom{m_i}{y_i}p_j^{y_i}(1-p_j)^{m_i-y_i}$ and sample $q_i$ from the discrete distribution $\sum_{j=1}^{k}\pi_{ij}\delta_j(\cdot)$

3. A stick-breaking procedure: $\pi_j = V_j \prod_{l=1}^{j-1}(1-V_l)$ except for $j=k$ for which $\pi_k = 1-\sum_{l=1}^{k-1} \pi_l$, 
but with $V_j\sim\mathcal{Beta}(1+z_{+j}, 1+\sum_{l=j+1}^{k}z_{+l})$ 


```{r echo=FALSE}
# Clear workspace
rm(list=setdiff(ls(), c("y", "m", "n")))
set.seed(12345)
```

```{r echo=TRUE}
# parameters
k <- 20
alpha <- 1
```

```{r echo=TRUE}
# sample p
sample_p <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    y_plus <- y %*% z
    m_plus <- m %*% z
    a <- alpha/k + y_plus
    b <- alpha*(k-1)/k + m_plus - y_plus    
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rbeta(1, a[j], b[j]) 
    }
    # return
    return (samples)    
}
# construct z matrix from the q's
z_matrix <- function(q){
    # input control
    stopifnot(length(q)==n)
    # function
    z <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       z[i,q[i]] <- 1
    }
    # return
    return (z)
}
# sample q's
sample_z <- function(pi, p){
    # input control
    stopifnot(length(p)==k)
    stopifnot(length(pi)==k)
    # function
    w <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       for (j in 1:k) {
        w[i,j] <- pi[j] * choose(m[i], y[i]) * p[j]^y[i] * (1-p[j])^(m[i]-y[i])
       }
    }
    q <- c()
    for (i in 1:n) {
       q[i] <- sample(1:k, size=1, prob=w[i,])
    }
    # return
    return (z_matrix(q))
}
# sample pi
sample_pi <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    z_plus <- colSums(z)
    a <- 1 + z_plus
    b <- c()
    for (j in 1:(k-1)) {
        b[j] <- 1 + sum(z_plus[(j+1):k])
    }
    b[k] <- 1
    # sample V's
    V <- c()
    for (j in 1:k) {
       V[j] <- rbeta(1, a[j], b[j])
    }
    pi <- c()
    pi[1] <- V[1]
    for (j in 2:(k-1)) {
       pi[j] <- V[j] * prod(1-V[1:(j-1)])
    }
    pi[k] <- max(0, 1 - sum(pi)) # to take care of numerical approximation errors
    # return
    return (pi)
}
```

```{r echo=TRUE}
# initial parameters (randomly generated)
pi_0 <- runif(k)
pi_0 <- pi_0 / sum(pi_0)
p_0 <- sort(runif(k))
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

```{r echo=TRUE}
# Gibbs sampler
gibbs_np <- function(n_samples, p_0, z_0, pi_0){

    # input control
    stopifnot(nrow(z_0)==n)
    stopifnot(ncol(z_0)==k)
    stopifnot(length(pi_0)==k)
    stopifnot(length(p_0)==k)

    # initial values
    pi_curr <- pi_0
    p_curr <- p_0
    z_curr <- z_0
  
    # parameters to be monitored
    pi_samples <- matrix(nrow = n_samples, ncol = k)
    p_samples <- matrix(nrow = n_samples, ncol = k)

    # iteration
    for (i in 1:n_samples) {
        pi_curr <- sample_pi(z_curr)
        p_curr <- sample_p(z_curr)
        z_curr <- sample_z(pi_curr, p_curr)
        # saving
        pi_samples[i,] <- pi_curr
        p_samples[i, ] <- p_curr
    }

    # return
    return (list(pi_samples=pi_samples, p_samples=p_samples))
}
```


### Run the non parametric Gibbs sampler for the above data and summurize your finding. In particular discuss the posterior distribution of the number of clusters

Perform sampling
```{r echo=TRUE}
n_samples <- 1000
samples <- gibbs_np(n_samples, p_0, z_0, pi_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("pi[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("p[", j, "]"))
}
samples <- cbind(samples$pi_samples, samples$p_samples)
colnames(samples) <- names
```

The effective number of clusters can be infereed from the following plot
```{r echo=FALSE}
plot(colSums(samples[,(1:k)])/n_samples, type="b", xlab="cluster", ylab="probability")
```

Summary

```{r echo=TRUE}
means <- colMeans(samples[,c(1:4,(k+1):(k+4))])
sds <- apply(samples[,c(1:4,(k+1):(k+4))], 2, sd)
summ <- round(rbind(means, sds),4)
colnames(summ) <- names[c(1:4,(k+1):(k+4))]
rownames(summ) <- c("mean", "sd")
kable(summ)
```

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

<!-- ------------------------------------------------------------------------------------------ -->

# Exercise 2: Mixture of Gaussians

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

```{r, eval=TRUE, echo=FALSE}
set.seed(1237)    
mu <- rnorm(4, mean=0, sd=5)
sigma <- rgamma(4, 4, 4)  
n <- rpois(4, lambda=30)
y <- unlist(sapply(1:4, function(i) rnorm(n[i], mean=mu[i], sd=sigma[i])))
```
  
The data consists in measurements from a Gaussian mixture where each of the component of the mixture is a univariate normal distribution with 
unknown mean and variance:

\begin{equation}
    f(y|w, \{ \mu_j, \tau_j \}_j) = \sum_{j=1}^{k} w_j N(y_i; \mu_j, \tau_j)
\end{equation}

Where $N(y; \mu, \tau)$ is the density of a normal distribution parametrized by the mean $\mu$ and the precision $\tau$.

### By graphical inspection guess the number of components and implement in R a Gibbs sampler based on data augmentation

```{r echo=FALSE}
plot(density(y), main="Density estimate of the given data", xlab="y")
```

In the previous plot three modes are present, thus there are at least three clusters. Let's assume $k=3$.

The complete likelihood for the mixture model is

\begin{equation}
    f(y_i | \{ \mu_j, \tau_j \}_j, z) = \prod_{j=1}^{k} N(y_i; \mu_j, \tau_j)^{z_{ij}}
\end{equation}

The rows of the data augmentation matrix are constructed extracting the column index from a categorical distribution, as done in the previous exercise.

Let the $w$'s be the weights of the clusters.
Assume the following prior structure:

\begin{align}
    \mu_j &\sim \mathcal{N}(m, t) \\
    \tau_j &\sim \mathcal{Gamma}(a, b) \\
    w &\sim \mathcal{Dirichlet}(c) 
\end{align}

Applying Bayes' theorem and calculating the full conditionals yields to:

1. Full conditional for $w$: $\mathcal{Dirichlet}(c_1+z_{+1}, \dots, c_k+z_{+k})$

2. Full conditional for each $\tau_j$: $\mathcal{Gamma}(a+z_{+j}/2, b + \sum_{i}z_{ij}(y_i-\mu_j)^2 / 2)$

3. Full conditional for each $\mu_j$: $\mathcal{N}(\frac{t m + \tau_jy_{+j}}{t + \tau_jz_{+j}}, t + \tau_jz_{+j})$

4. Full conditional for each cluster-indicator variable: $\mathcal{Cat}(r_{i1}, \dots, r_{ik})$, where $r_{ij} \propto w_j N(y_i; \mu_j, \tau_j)$

```{r echo=TRUE}
k <- 3
n <- length(y)
# hyper-parameters for the gamma distribution
a <- 1
b <- 1
# hyper-parameters for the Dirichlet distribution
c <- rep(1,k)
# hyper-parameters for the normal distribution
m <- 0
t <- 1e-3
# initial parameters
mu_0 <- quantile(y, probs = seq(0, 1, length.out = k))
tau_0 <- rep(1e-2, k)
w_0 <- rep(1/k, k)
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

```{r echo=TRUE}
library(DirichletReg)
library(knitr)
# sampling from full conditional for w
sample_w <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    z_plus <- colSums(z)
    c_new <- c + z_plus
    # return
    rdirichlet(1, c_new)   
}

# sampling from full conditional for mu
sample_mu <- function(z, tau){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    stopifnot(length(tau)==k)
    # function
    y_plus <- y %*% z
    z_plus <- colSums(z)
    tau_new <- tau*z_plus + t
    mu_new <- (tau*y_plus + t*m) / tau_new
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rnorm(1, mu_new[j], 1/sqrt(tau_new[j])) 
    }
    # return
    return (samples)    
}

# sampling from full conditional for tau
sample_tau <- function(z, mu){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    stopifnot(length(mu)==k)
    # function
    y_plus <- y %*% z
    y2_plus <- (y^2) %*% z
    z_plus <- colSums(z)
    a_new <- a + z_plus/2
    b_new <- b + (y2_plus + 2*mu*y_plus + z_plus*mu^2) / 2
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rgamma(1, a_new[j], b_new[j]) 
    }
    # return
    return (samples)    
}

# sampling from full conditional for z
sample_z <- function(w, mu, tau){
    # input control
    stopifnot(length(w)==k)
    stopifnot(length(mu)==k)
    stopifnot(length(tau)==k)
    # function
    r <- matrix(nrow=n, ncol=k)
    for (i in 1:n) {
       for (j in 1:k) {
          r[i,j] <- w[j] * dnorm(y[i], mu[j], 1/sqrt(tau[j]))
       }
    }
    q <- rep(0,n)
    for (i in 1:n) {
       q[i] <- sample(1:k, size=1, prob=r[i,])
    }
    z <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       z[i,q[i]] <- 1
    }
    # return
    return (z)    
}
```

```{r echo=TRUE}
# Gibbs sampler
gibbs <- function(n_samples, w_0, mu_0, tau_0, z_0){

    # input control
    stopifnot(nrow(z_0)==n)
    stopifnot(ncol(z_0)==k)
    stopifnot(length(w_0)==k)
    stopifnot(length(mu_0)==k)
    stopifnot(length(tau_0)==k)

    # initial values
    w_curr <- w_0
    mu_curr <- mu_0
    tau_curr <- tau_0
    z_curr <- z_0
  
    # parameters to be monitored
    w_samples <- matrix(nrow = n_samples, ncol = k)
    mu_samples <- matrix(nrow = n_samples, ncol = k)
    tau_samples <- matrix(nrow = n_samples, ncol = k)

    # iteration
    for (i in 1:n_samples) {
        # update
        w_curr <- sample_w(z_curr)
        mu_curr <- sample_mu(z_curr, tau_curr)
        tau_curr <- sample_tau(z_curr, mu_curr)
        z_curr <- sample_z(w_curr, mu_curr, tau_curr)
        # label switching
        order <- order(mu_curr)
        w_curr <- w_curr[order]
        mu_curr <- mu_curr[order]
        tau_curr <- tau_curr[order]
        z_curr <- z_curr[,order]
        # saving
        w_samples[i,] <- w_curr
        mu_samples[i, ] <- mu_curr
        tau_samples[i, ] <- tau_curr
    }

    # return
    return (list(w_samples=w_samples, mu_samples=mu_samples, tau_samples=tau_samples))    
}
```

```{r echo=TRUE}
simulation_plots <- function(n_samples, samples, k){

    # input control
    stopifnot(nrow(samples)==n_samples)
    stopifnot(ncol(samples)==3*k)

    # MCMC plots
    par(mfrow = c(3, k))
    for (j in 1:k) {
        plot(1:n_samples,samples[,j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[j])
    }
    for (j in 1:k) {
        plot(1:n_samples,samples[,k+j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[k+j])   
    }
    for (j in 1:k) {
        plot(1:n_samples,samples[,2*k+j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[2*k+j])   
    }
    # Restore default plot options
    par(mfrow = c(1, 1))

    # Histograms
    par(mfrow = c(3, k))   
    for (j in 1:k) {
        hist(samples[,j], main=colnames(samples)[j], xlab=NULL)
    }
    for (j in 1:k) {
        hist(samples[,k+j], main=colnames(samples)[k+j], xlab=NULL)
    }
    for (j in 1:k) {
        hist(samples[,2*k+j], main=colnames(samples)[2*k+j], xlab=NULL)
    }
    # Restore default plot options
    par(mfrow = c(1, 1))
}

simulation_summary <- function(samples){
    # Summary
    means <- apply(samples, 2, mean)
    sds <- apply(samples, 2, sd)
    summ <- rbind(means, sds)
    colnames(summ) <- names
    rownames(summ) <- c("mean", "sd")
    kable(round(summ,4))
}
```


### Run the Gibbs sampler for the above data and summurize your finding

Perform sampling

```{r echo=TRUE}
n_samples <- 1000
burn_in <- 100
samples <- gibbs(n_samples+burn_in, w_0, mu_0, tau_0, z_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("w[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("mu[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("tau[", j, "]"))
}
samples_gibbs <- cbind(samples$w_samples, samples$mu_samples, samples$tau_samples)
samples_gibbs <- samples_gibbs[-(1:burn_in),]
colnames(samples_gibbs) <- names
```

```{r echo=TRUE}
simulation_plots(n_samples, samples_gibbs, k)
```

### Use JAGS to implement the same model and compare the results

```{r echo=TRUE}
data_list <- list(
  "y" = y,
  "n" = n,
  "k" = k
)
str(data_list)
```

```{r echo=TRUE}
library(rjags)
library(R2jags)

# Define the model
normal_mixture <- "model {
    
    # noninformative hyperpriors 
    a ~ dunif(0, 100)
    b ~ dunif(0, 100)
    m ~ dunif(-100, 100)
    t ~ dunif(0, 100)
    for (j in 1:k) {
        c[j] ~ dunif(0, 100)
    }

    # prior on alpha
    w ~ ddirch(c[])
    
    # prior on q
    for (i in 1:n){
        q[i] ~ dcat(w[])
    }

    # prior on mu
    for (j in 1:k) {
        mu[j] ~ dnorm(m, t)
    }

    # prior on tau
    for (j in 1:k) {
        tau[j] ~ dgamma(a, b)
    }

    # Likelihood
    for (i in 1:n) { 
        y[i] ~ dnorm(mu[q[i]], tau[q[i]])
    }

}"

# Specify desired parameters for inference
parameters <- c("w", "mu", "tau")

# Run
n_samples <- 1000
n.burnin <- 100
n.thin   <- 1

MCMCrun <- jags(
  data = data_list,
  parameters.to.save = parameters,
  model.file = textConnection(normal_mixture),
  n.burnin = n.burnin,
  n.iter = n.thin*n_samples + n.burnin,
  n.chains = 1,
  n.thin = n.thin,
  progress.bar = "none",
  quiet = TRUE,
  DIC = FALSE
)
```

```{r echo=TRUE}
# Posterior samples extraction
samples_list <- MCMCrun$BUGSoutput$sims.list
str(samples_list)
# Convert list to matrix
samples_jags <- cbind(samples_list$w, samples_list$mu, samples_list$tau)
# names
colnames(samples_jags) <- names
```

```{r echo=TRUE}
simulation_plots(n_samples, samples_jags, k)
```

#### Comparison 

```{r echo=TRUE}
simulation_summary(samples_gibbs)
simulation_summary(samples_jags)
```

The results obtained with JAGS are expected to be more reliable, as it is possible to conclude from the trace plots.
However, similar results are obtained, even if not so precisely. 
In particular, the implement version of the Gibbs sampler requires more samples to properly work, instead jags is optimized.


### Assume the number of components is actually one more then the one you guess. Repeat the analysis using both your algorithm and JAGS

```{r echo=TRUE}
k <- 4
# re-initialize what needed
c <- rep(1,k)
w_0 <- rep(1/k, k)
mu_0 <- quantile(y, probs = seq(0, 1, length.out = k))
tau_0 <- rep(1e-2, k)
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

Gibbs sampler

```{r echo=TRUE}
samples <- gibbs(n_samples+burn_in, w_0, mu_0, tau_0, z_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("w[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("mu[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("tau[", j, "]"))
}
samples_gibbs <- cbind(samples$w_samples, samples$mu_samples, samples$tau_samples)
samples_gibbs <- samples_gibbs[-(1:burn_in),]
colnames(samples_gibbs) <- names
```

```{r echo=TRUE}
simulation_plots(n_samples, samples_gibbs, k)
```

JAGS

```{r echo=TRUE}
data_list$k <- k
str(data_list)

MCMCrun <- jags(
  data = data_list,
  parameters.to.save = parameters,
  model.file = textConnection(normal_mixture),
  n.burnin = n.burnin,
  n.iter = n.thin*n_samples + n.burnin,
  n.chains = 1,
  n.thin = n.thin,
  progress.bar = "none",
  quiet = TRUE,
  DIC = FALSE
)
```

```{r echo=TRUE}
# Posterior samples extraction
samples_list <- MCMCrun$BUGSoutput$sims.list
str(samples_list)
# Convert list to matrix
samples_jags <- cbind(samples_list$w, samples_list$mu, samples_list$tau)
colnames(samples_jags) <- names
```

```{r echo=TRUE}
simulation_plots(n_samples, samples_jags, k)
```

#### Comparison 

```{r echo=TRUE}
simulation_summary(samples_gibbs)
simulation_summary(samples_jags)
```


### Assume now the number of components is unknown. Implement in R a non parametric Gibbs sampler with stick-breaking approach and Dirichlet prior

In this case the model is:

\begin{align}
    y_i | q_i, \{ \mu_j, \tau_j \}_j &\sim \mathcal{N}(\mu_{q_i},\tau_{q_i}) \\
    \mu_j &\sim \mathcal{N}(m, t) \\
    \tau_j &\sim \mathcal{Gamma}(a.b) \\
    q_i | w &\sim \mathcal{Cat}(w_1, \dots, w_k) \\
    w &\sim GEM(\alpha)
\end{align}

#### Sample from the posterior: Blocked Gibbs sampler.

The state space of the Gibbs sampler is $\{ \mu=(\mu_1, \dots, \mu_k), \tau=(\tau_1, \dots, \tau_k), w=(w_1, \dots, w_k), q_1, \dots, q_n \}$. 
The sampling iteration procedes as follows:

1. Sample $\mu | \tau, q_1, \dots, q_n, y$

2. Sample $\tau | \mu, q_1, \dots, q_n, y$

3. Sample $q_1, \dots, q_n | \mu, \tau, w, y$

4. Sample $w | q_1, \dots, q_n$

Where:

1. Exploiting conjugacy, each $\mu_j$ is sampled from a $\mathcal{Normal}(\frac{mt+y_{+j}\tau_j}{t+z_{+j}\tau_j},t+z_{+j}\tau_j)$

2. Exploiting conjugacy, each $\tau_j$ is sampled from a $\mathcal{Gamma}(a+z_{+j}/2, b+\sum_i z_{ij}(y_i-\mu_j)^2 /2)$

3. Similarly to the exercise 1, but with $w_{ij} \propto w_i N(y_i; \mu_j, \tau_j)$

4. A stick-breaking procedure as done in exercise 1

```{r echo=FALSE}
# Clear workspace
rm(list=setdiff(ls(), c("y")))
n <- length(y)
set.seed(12345)
```

```{r echo=TRUE}
# parameters
k <- 20
```

```{r echo=TRUE}
# sample mu
sample_mu <- function(z, tau){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    y_plus <- y %*% z
    z_plus <- colSums(z)
    t_new <- t + z_plus*tau 
    m_new <- (m*t + y_plus*tau) / t_new
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rnorm(1, m_new[j], 1/sqrt(t_new[j])) 
    }
    # return
    return (samples)    
}
# sample tau
sample_tau <- function(z, mu){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    y_plus <- y %*% z
    y2_plus <- (y^2) %*% z
    z_plus <- colSums(z)
    a_new <- a + z_plus/2 
    b_new <- b + (y2_plus -2*mu*y_plus + z_plus*mu^2) / 2
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rgamma(1, a_new[j], b_new[j]) 
    }
    # return
    return (samples)    
}
# construct z matrix from the q's
z_matrix <- function(q){
    # input control
    stopifnot(length(q)==n)
    # function
    z <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       z[i,q[i]] <- 1
    }
    # return
    return (z)
}
# sample q's
sample_z <- function(w, mu, tau){
    # input control
    stopifnot(length(mu)==k)
    stopifnot(length(tau)==k)
    stopifnot(length(w)==k)
    # function
    r <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       for (j in 1:k) {
        r[i,j] <- w[j] * dnorm(y[i], mu[j], 1/sqrt(tau[j]))
       }
    }
    q <- c()
    for (i in 1:n) {
       q[i] <- sample(1:k, size=1, prob=r[i,])
    }
    # return
    return (z_matrix(q))
}
# sample w
sample_w <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    z_plus <- colSums(z)
    aa <- 1 + z_plus
    bb <- c()
    for (j in 1:(k-1)) {
        bb[j] <- 1 + sum(z_plus[(j+1):k])
    }
    bb[k] <- 1
    # sample V's
    V <- c()
    for (j in 1:k) {
       V[j] <- rbeta(1, aa[j], bb[j])
    }
    w <- c()
    w[1] <- V[1]
    for (j in 2:(k-1)) {
       w[j] <- V[j] * prod(1-V[1:(j-1)])
    }
    w[k] <- max(0, 1 - sum(w)) # to take care of numerical approximation errors
    # return
    return (w)
}
```

```{r echo=TRUE}
# hyper-parameters for the gamma distribution
a <- 1
b <- 1
# hyper-parameters for the normal distribution
m <- 0
t <- 1e-3
# initial values for the chain
mu_0 <- quantile(y, probs = seq(0, 1, length.out = k))
tau_0 <- rep(1e-2, k)
w_0 <- rep(1/k, k)
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

```{r echo=TRUE}
# Gibbs sampler
gibbs_np <- function(n_samples, mu_0, tau_0, z_0, w_0){

    # input control
    stopifnot(nrow(z_0)==n)
    stopifnot(ncol(z_0)==k)
    stopifnot(length(w_0)==k)
    stopifnot(length(mu_0)==k)
    stopifnot(length(tau_0)==k)

    # initial values
    w_curr <- w_0
    mu_curr <- mu_0
    tau_curr <- tau_0
    z_curr <- z_0
  
    # parameters to be monitored
    w_samples <- matrix(nrow = n_samples, ncol = k)
    mu_samples <- matrix(nrow = n_samples, ncol = k)
    tau_samples <- matrix(nrow = n_samples, ncol = k)

    # iteration
    for (i in 1:n_samples) {
        w_curr <- sample_w(z_curr)
        mu_curr <- sample_mu(z_curr, tau_curr)
        tau_curr <- sample_tau(z_curr, mu_curr)
        z_curr <- sample_z(w_curr, mu_curr, tau_curr)
        # saving
        w_samples[i,] <- w_curr
        mu_samples[i, ] <- mu_curr
        tau_samples[i, ] <- tau_curr
    }

    # return
    return (list(w_samples=w_samples, mu_samples=mu_samples, tau_samples=tau_samples))
}
```


### Run the non parametric Gibbs sampler for the above data and summurize your finding. In particular discuss the posterior distribution of the number of clusters

Perform sampling
```{r echo=TRUE}
n_samples <- 1000
samples <- gibbs_np(n_samples, mu_0, tau_0, z_0, w_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("w[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("mu[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("tau[", j, "]"))
}
samples <- cbind(samples$w_samples, samples$mu_samples, samples$tau_samples)
colnames(samples) <- names
```

The effective number of clusters can be infereed from the following plot
```{r echo=FALSE}
plot(colSums(samples[,(1:k)])/n_samples, type="b", xlab="cluster", ylab="probability")
```

Summary

```{r echo=TRUE}
means <- colMeans(samples[,c(1:4,(k+1):(k+4),(2*k+1):(2*k+4))])
sds <- apply(samples[,c(1:4,(k+1):(k+4),(2*k+1):(2*k+4))], 2, sd)
summ <- round(rbind(means, sds),4)
colnames(summ) <- names[c(1:4,(k+1):(k+4),(2*k+1):(2*k+4))]
rownames(summ) <- c("mean", "sd")
kable(summ)
```