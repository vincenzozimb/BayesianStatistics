---
title: Homework 4 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
---

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

# Exercise 1: Mixture of binomials

```{r eval=TRUE, echo=FALSE}
set.seed(1236)    
theta <- runif(4)
m <- rpois(40, lambda=30)
p <- sample(theta, size=40, replace=TRUE)
y <- rbinom(40, size=m, prob=p)      
```
In the below table there are $40$ observations $y_i \sim Bin(m_i, p_i)$.

```{r eval=TRUE, echo=FALSE}
library(knitr)
x <- cbind(m,y)
colnames(x) <- c("m", "y")
kable(t(x))
```

### Assume that the number of distinct $p_i$s is $4$. Implement in R a Gibbs sampler based on data augmentation

Let $n$ be the number of observations, $k$ the number of classes. The index $i$ runs in $1, \dots, n$ while the index $j$ in $1, \dots, k$.

For every $i$, let $q_i \in {1, \dots, k}$ and $q_i=j$ iff data $y_i$ belongs to class $j$. 
The data augmentation matrix has components $z_{ij}=1$ iff $q_i=j$, and $z_{ij}=0$ otherwise. 

The complete likelihood is:

\begin{align}
    f(y | \alpha, p, z) &= \prod_{i=1}^{n} \binom{m_i}{y_i} p_{q_i}^{y_i}(1-p_{q_i})^{m_i-y_i} \\
    &= \prod_{i=1}^{n} \binom{m_i}{y_i} \prod_{j=1}^{k} [p_j^{y_i}(1-p_j)^{m_i-y_i}]^{z_{ij}} 
\end{align}

Prior on $z$. The row of $z$ are independent and constructed assuming a categorical distribution as a model, leading to:

\begin{equation}
    f(z | \alpha) = \prod_{i=1}^{n} \prod_{j=1}^{k} \alpha_i^{z_{ij}}
\end{equation}

Prior on $\alpha$. As the alphas form a discrete probability distribution, the vector $\alpha$ must belong to the simplex of order $k-1$. 
Thus we choose as a prior distribution a Dirichlet distribution:

\begin{equation}
    f(\alpha) \propto \prod_{j=1}^{k} \alpha_j^{c_j-1}
\end{equation}

Prior on $p$. We assume each $p$ to be independent and distributed according to a beta distribution:

\begin{equation}
    f(p) \propto \prod_{j=1}^{k} p_j^{a_j-1}(1-p_j)^{b_j-1}
\end{equation}

Applying Bayes' theorem, the posterior distribution is:

\begin{equation}
    f(\alpha, p, z) \propto f(y | \alpha, p, z) f(z | \alpha) f(\alpha) f(p)
\end{equation}

In order to implement a Gibbs sampler, we need the full conditionals. Exploiting conjugacy we obtain:

\begin{align}
    \alpha | p, z, y &\sim \mathcal{Dir}(c_1+z_{+1}, \dots, c_k+z_{+k}) \\
    p_j | \alpha, z, y, &\sim \mathcal{Beta}(a_j+y_{+j}, b_j+m_{+j}-y_{+j}) \\
    \mathbb{P}(z_{ij}=1 | \alpha, p, y) &\propto \alpha_j p_j^{y_i} (1-p_j)^{m_i-y_i} =: r_{ij} 
\end{align}

where $z_{+j}:=\sum_{i=1}^{n}z_{ij}$, $y_{+j}:=\sum_{i=1}^{n}z_{ij}y_i$ and $m_{+j}:=\sum_{i=1}^{n}z_{ij}m_i$.
The meaning of the last one is $q_i | \alpha, p, y \sim \mathcal{Cat}(r_{i1}, \dots, r_{ik})$.

```{r echo=TRUE}
k <- 4
n <- length(y)  
# hyper-parameters for the beta distribution
a <- rep(0.5,k)
b <- rep(0.5,k)
# hyper-parameters for the Dirichlet distribution
c <- rep(1,k)
# initial parameters (randomly generated)
alpha_0 <- runif(k)
alpha_0 <- alpha_0 / sum(alpha_0)
p_0 <- sort(runif(k))
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

```{r echo=TRUE}
library(DirichletReg)
# sampling from full conditional for alpha
sample_alpha <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    z_plus <- colSums(z)
    c_new <- c + z_plus
    # return
    rdirichlet(1, c_new)
}

# sampling from full conditional for p
sample_p <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    y_plus <- y %*% z
    m_plus <- m %*% z
    a_new <- a + y_plus
    b_new <- b + m_plus - y_plus    
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rbeta(1, a_new[j], b_new[j]) 
    }
    # return
    return (samples)    
}

# sampling from full conditional for z
sample_z <- function(alpha, p){
    # input control
    stopifnot(length(alpha)==k)
    stopifnot(length(p)==k)
    # function
    r <- matrix(nrow=n, ncol=k)
    for (i in 1:n) {
       for (j in 1:k) {
          r[i,j] <- alpha[j] * p[j]^y[i] * (1-p[j])^(m[i]-y[i])
       }
    }
    f <- rep(0,n)
    for (i in 1:n) {
       f[i] <- sample(1:k, size=1, prob=r[i,])
    }
    z <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       z[i,f[i]] <- 1
    }
    # return
    return (z)    
}

# Gibbs sampler
gibbs <- function(n_samples, alpha_0, p_0, z_0){

    # input control
    stopifnot(nrow(z_0)==n)
    stopifnot(ncol(z_0)==k)
    stopifnot(length(alpha_0)==k)
    stopifnot(length(p_0)==k)

    # initial values
    alpha_curr <- alpha_0
    p_curr <- p_0
    z_curr <- z_0
  
    # parameters to be monitored
    alpha_samples <- matrix(nrow = n_samples, ncol = k)
    p_samples <- matrix(nrow = n_samples, ncol = k)

    # iteration
    for (i in 1:n_samples) {
        alpha_curr <- sample_alpha(z_curr)
        p_curr <- sample_p(z_curr)
        z_curr <- sample_z(alpha_curr, p_curr)
        # saving
        alpha_samples[i,] <- alpha_curr
        p_samples[i, ] <- p_curr
    }

    # return
    return (list(alpha_samples=alpha_samples, p_samples=p_samples))    
}
```

### Run the Gibbs sampler for the above data and summurize your finding

Perform sampling

```{r echo=TRUE}
n_samples <- 1000
samples <- gibbs(n_samples, alpha_0, p_0, z_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("alpha[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("p[", j, "]"))
}
samples <- cbind(samples$alpha_samples, samples$p_samples)
colnames(samples) <- names
```

MCMC plots 

```{r echo=FALSE, eval=TRUE}
# trace  to check convergence 
par(mfrow = c(1, k))
for (j in 1:k) {
    plot(1:n_samples,samples[,j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[j])
}
for (j in 1:k) {
    plot(1:n_samples,samples[,k+j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[k+j])   
}
# Restore default plot options
par(mfrow = c(1, 1))
# correlation
# acf(samples)
```

Histograms

```{r echo=FALSE}
par(mfrow = c(1, k))
for (j in 1:k) {
    hist(samples[,j], main=colnames(samples)[j], xlab=NULL)
}
for (j in 1:k) {
    hist(samples[,k+j], main=colnames(samples)[k+j], xlab=NULL)
}
# Restore default plot options
par(mfrow = c(1, 1))
```

Summary

```{r echo=TRUE}
means <- colMeans(samples)
sds <- apply(samples, 2, sd)
summ <- rbind(means, sds)
colnames(summ) <- names
rownames(summ) <- c("mean", "sd")
kable(summ)
```



### Use JAGS to implement the same model and compare the results  

```{r echo=TRUE}
data_list <- list(
  "y" = y,
  "m" = m,
  "n" = n,
  "k" = k,
  "a" = a,
  "b" = b,
  "c" = c
)
str(data_list)
```

```{r echo=TRUE}
library(rjags)
library(R2jags)

# Define the model
binomial_mixture <- "model {
    # prior on alpha
    alpha ~ ddirch(c[])
    
    # prior on q
    for (i in 1:n){
        q[i] ~ dcat(alpha[])
    }

    # prior on p
    for (j in 1:k) {
        p[j] ~ dbeta(a[j], b[j])
    }

    # Likelihood
    for (i in 1:n) { 
        y[i] ~ dbin(p[q[i]], m[i])
    }

}"

# Specify desired parameters for inference
parameters <- c("alpha", "p")

# Run
n_samples <- 1000
n.burnin <- 0
n.thin   <- 1

MCMCrun <- jags(
  data = data_list,
  parameters.to.save = parameters,
  model.file = textConnection(binomial_mixture),
  n.burnin = n.burnin,
  n.iter = n.thin*n_samples + n.burnin,
  n.chains = 1,
  n.thin = n.thin,
  progress.bar = "none",
  quiet = TRUE,
  DIC = FALSE
)
```

```{r echo=TRUE}
# Posterior samples extraction
samples_list <- MCMCrun$BUGSoutput$sims.list
str(samples_list)
# Convert list to matrix
samples <- cbind(samples_list$alpha, samples_list$p)
names <- c()
for (i in 1:k) {
   names <- c(names, paste0("alpha[", i, "]"))
}
for (i in 1:k) {
   names <- c(names, paste0("p[", i, "]"))
}
colnames(samples) <- names
# Define the maximum number of rows to display
max_row <- 4
# Display a subset of rows and columns from the large matrix
subset_matrix <- samples[1:max_row, ]
round(subset_matrix, 4)
```

MCMC plots 

```{r echo=FALSE, eval=TRUE}
# trace  to check convergence 
par(mfrow = c(1, k))
for (j in 1:k) {
    plot(1:n_samples,samples[,j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[j])
}
for (j in 1:k) {
    plot(1:n_samples,samples[,k+j], type="l", main=NULL, xlab = expression(i), ylab = colnames(samples)[k+j])   
}
# Restore default plot options
par(mfrow = c(1, 1))
# correlation
# acf(samples)
```

Histograms

```{r echo=FALSE}
par(mfrow = c(1, k))
for (j in 1:k) {
    hist(samples[,j], main=colnames(samples)[j], xlab=NULL)
}
for (j in 1:k) {
    hist(samples[,k+j], main=colnames(samples)[k+j], xlab=NULL)
}
# Restore default plot options
par(mfrow = c(1, 1))
```

Summary

```{r echo=TRUE}
summ <- MCMCrun$BUGSoutput$summary
summ <- round(summ[,c(1,2)],4)
kable(t(summ))
```


#### Comparison

The results obtained with JAGS are expected to be more reliable. 
The differences in the results are due to the correlation stucture of the sample generated with the implemented version of the Gibbs sampler.
It can be notice also from the trace plots that the implemented version of the Gibbs sampler explore the configuration space in a weaker way:
it seems that $2-3$ weakly connected regions are present, leading indeed to multimodal histograms for $p$.
JAGS is able to efficienlty sample also the intermediate (of lower probability) regions. 


### Assume now the number of components (distinct $p_i$) is unknown. Implement in R a non parametric Gibbs sampler with stick-breaking approach and Dirichlet prior

The stick-breaking construction of a Dirichlet process with base measure $H$ and concentration parameter $\alpha$ is as follows:

\begin{align}
    \beta_k &\sim \mathcal{Beta}(1,\alpha) \\
    \pi_k &:= \beta_k \prod_{j=1}^{k-1}(1-\beta_j) \Leftrightarrow \pi \sim GEM(\alpha)  \\
    \theta_k &\sim H
\end{align}

Then the random probability measure defined as $G:=\sum_{k=1}^{\infty}\pi_k \delta_{\theta_k}$ is distributed according to $G \sim DP(\alpha, H)$.


A non-parametric bayesian mixture model is a model in the form:

\begin{align}
    y_i | \theta_i &\sim F_{\theta_i} \\
    \theta_i | G &\sim G \\
    G | \alpha, H &\sim DP(\alpha, H)
\end{align}

Being $G$ a realization of a Dirichlet process, it is a discrete probability distribution. This means that multiple $\theta_i$'s can take the 
same value simultaneously, hence the interpretation of the model as a mixture model.

We can rewrite the model using the stick-breaking construction. Let $z_i$ be a class assigment variable, taking value $j$ with probability $\pi_j$:

\begin{align}
    y_i | z_i, \{ \theta_j \}_j &\sim F_{\theta_{z_i}} \\
    \theta_j &\sim H \\
    z_i | \pi &\sim \mathcal{Cat}(\pi) \\
    \pi &\sim GEM(\alpha)
\end{align}

From a practical point of view, we need to fix a sufficiently large upper bound $k$ and truncate the random measure $G$.
In this way we obtain an approximate representation of the Dirichlet process.
Of course the worst case scenario (complete absence of any prior information) would require at most to fix $k$ to the number of data. 
The algorithm will select, based on the data, only the relevant classes, according to the non parametric paradigm. 

In our model:

\begin{align}
    y_i | q_i, \{ p_j \}_j &\sim \mathcal{Bin}(m_i,p_{q_i}) \\
    p_j &\sim \mathcal{Dir}(\alpha/k, \dots, \alpha/k) \\
    q_i | \pi &\sim \mathcal{Cat}(\pi_1, \dots, \pi_k) \\
    \pi &\sim GEM(\alpha)
\end{align}

#### Sample from the posterior: Blocked Gibbs sampler.

The state space of the Gibbs sampler is $\{ p=(p_1, \dots, p_k), \pi=(\pi_1, \dots, \pi_k), q_1, \dots, q_n \}$. 
The sampling iteration procedes as follows:

1. Sample $p | q_1, \dots, q_n, y$

2. Sample $q_1, \dots, q_n | p, \pi, y$

3. Sample $\pi | q_1, \dots, q_n$

Where:

1. Exploiting conjugacy, each $p_j$ is sampled from a $\mathcal{Beta}(\alpha/k + y_{+j}, \alpha(k-1)/k + m_{+j} - y_{+j})$

2. For each $i$, calculate $\pi_{ij} \propto \pi_j \binom{m_i}{y_i}p_j^{y_i}(1-p_j)^{m_i-y_i}$ and sample $q_i$ from the discrete distribution $\sum_{j=1}^{k}\pi_{ij}\delta_k(\cdot)$

3. A stick-breaking procedure: $\pi_j = V_j \prod_{l=1}^{j-1}(1-V_l)$ except for $j=k$ for which $\pi_k = 1-\sum_{l=1}^{k-1} \pi_l$, 
but with $V_j\sim\mathcal{Beta}(1+z_{+j}, 1+\sum_{l=j+1}^{k}z_{+l})$ 


```{r echo=FALSE}
# Clear workspace
rm(list=setdiff(ls(), c("y", "m", "n")))
set.seed(12345)
```

```{r echo=TRUE}
# parameters
k <- 20
alpha <- 1
```

```{r echo=TRUE}
# sample p
sample_p <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    y_plus <- y %*% z
    m_plus <- m %*% z
    a <- alpha/k + y_plus
    b <- alpha*(k-1)/k + m_plus - y_plus    
    samples <- c()
    for (j in 1:k) {
       samples[j] <- rbeta(1, a[j], b[j]) 
    }
    # return
    return (samples)    
}
# construct z matrix from the q's
z_matrix <- function(q){
    # input control
    stopifnot(length(q)==n)
    # function
    z <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       z[i,q[i]] <- 1
    }
    # return
    return (z)
}
# sample q's
sample_z <- function(pi, p){
    # input control
    stopifnot(length(p)==k)
    stopifnot(length(pi)==k)
    # function
    w <- matrix(0, nrow=n, ncol=k)
    for (i in 1:n) {
       for (j in 1:k) {
        w[i,j] <- pi[j] * choose(m[i], y[i]) * p[j]^y[i] * (1-p[j])^(m[i]-y[i])
       }
    }
    q <- c()
    for (i in 1:n) {
       q[i] <- sample(1:k, size=1, prob=w[i,])
    }
    # return
    return (z_matrix(q))
}
# sample pi
sample_pi <- function(z){
    # input control
    stopifnot(nrow(z)==n)
    stopifnot(ncol(z)==k)
    # function
    z_plus <- colSums(z)
    a <- 1 + z_plus
    b <- c()
    for (j in 1:(k-1)) {
        b[j] <- 1 + sum(z_plus[(j+1):k])
    }
    b[k] <- 1
    # sample V's
    V <- c()
    for (j in 1:k) {
       V[j] <- rbeta(1, a[j], b[j])
    }
    pi <- c()
    pi[1] <- V[1]
    for (j in 2:(k-1)) {
       pi[j] <- V[j] * prod(1-V[1:(j-1)])
    }
    pi[k] <- max(0, 1 - sum(pi)) # to take care of numerical approximation errors
    # return
    return (pi)
}
```

```{r echo=TRUE}
# initial parameters (randomly generated)
pi_0 <- runif(k)
pi_0 <- pi_0 / sum(pi_0)
p_0 <- sort(runif(k))
z_0 <- matrix(0, nrow=n, ncol=k)
for (i in 1:n) {
   idx <- sample(1:k, 1)
   z_0[i, idx] <- 1
}
```

```{r echo=TRUE}
# # test functions
# test_p <- sample_p(z_0)
# test_z <- sample_z(pi_0, p_0)
# test_pi <- sample_pi(z_0)
```

```{r echo=TRUE}
# Gibbs sampler
gibbs_np <- function(n_samples, p_0, z_0, pi_0){

    # input control
    stopifnot(nrow(z_0)==n)
    stopifnot(ncol(z_0)==k)
    stopifnot(length(pi_0)==k)
    stopifnot(length(p_0)==k)

    # initial values
    pi_curr <- pi_0
    p_curr <- p_0
    z_curr <- z_0
  
    # parameters to be monitored
    pi_samples <- matrix(nrow = n_samples, ncol = k)
    p_samples <- matrix(nrow = n_samples, ncol = k)

    # iteration
    for (i in 1:n_samples) {
        pi_curr <- sample_pi(z_curr)
        p_curr <- sample_p(z_curr)
        z_curr <- sample_z(pi_curr, p_curr)
        # saving
        pi_samples[i,] <- pi_curr
        p_samples[i, ] <- p_curr
    }

    # return
    return (list(pi_samples=pi_samples, p_samples=p_samples))
}
```


### Run the non parametric Gibbs sampler for the above data and summurize your finding. In particular discuss the posterior distribution of the number of clusters

Perform sampling
```{r echo=TRUE}
n_samples <- 1000
samples <- gibbs_np(n_samples, p_0, z_0, pi_0)
str(samples)
# convert into matrix
names <- c()
for (j in 1:k) {
   names <- c(names, paste0("pi[", j, "]"))
}
for (j in 1:k) {
   names <- c(names, paste0("p[", j, "]"))
}
samples <- cbind(samples$pi_samples, samples$p_samples)
colnames(samples) <- names
```

The effective number of clusters can be infereed from the following plot
```{r echo=FALSE}
plot(colSums(samples[,(1:k)])/n_samples, type="b", xlab="cluster", ylab="probability")
```