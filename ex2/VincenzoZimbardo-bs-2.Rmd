---
title: Homework 2 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
bibliography: references.bib
---

# Exercise 1: Estimating a log-odds with a normal prior

Suppose $Y$ has a binomial distribution with parameters $n$ and $p$, and we are interesting in the log-odds value $\theta=\operatorname{logit}(p) = \log(p/(1-p))$. Our prior for $\theta$ is that $\theta \sim N(\mu, \sigma)$ (parametrized as mean and standard deviation). It follows that the posterior density of $\theta$ is given, up to a proportional constant, by

$$
g(\theta \vert y) \propto \frac{\exp(y\theta)}{(1+\exp(\theta))^n} \exp\left[ -\frac{(\theta-\mu)^2}{2 \sigma^2} \right]
$$  

More concretely, suppose we are interested in learning about the probability that a special coin lands heads when tossed. A priori we believe that the coin is fair, so we assign $\theta$ an $N(0, .25)$ prior. We toss the coin $n=5$ times and obtain $y=5$ heads.

- Using the prior density as a proposal density, design an Accept-Reject algorithm for sampling from the posterior distribution. Using simulated draws from your algorithm, approximate the probability that the coin is biased toward heads.

- Using the prior density as a proposal density, simulate from the posterior distribution using a Sampling Importance Resampling (SIR) algorithm. Approximate the probability that the coin is biased toward heads.
- Use a Laplace approximation to estimate the probability that the coin is biased toward heads. 
- In all above cases construct a $95\%$ probability interval.


```{r echo=FALSE}  
# Initial seed
set.seed(12345)
```
```{r echo=TRUE}  
# Parameters
mu <- 0
sigma <- 0.25
n_data <- 5
y_data <- 5
```
```{r echo=TRUE}  
# Posterior kernel
PosteriorKernel <- function(theta){
    ker <- exp(y_data*theta) / (1+exp(theta))^n_data
    ker <- ker * exp(-(theta-mu)^2/(2*sigma^2))
    return (ker)
}  
# Prior kernel as a proposal distribution
Proposal <- function(theta){
    ker <- exp(-(theta-mu)^2/(2*sigma^2))
    return (ker)
}
# Prior normalization
Z <- sqrt(2*pi*sigma^2)
# Sample from prior
PriorSample <- function(n=1){
    x <- rnorm(n,mu,sigma)
    return (x)
}
```
The aim is to estimate the probability that the coin is biased toward head. This means that we want to estimate the following quantity:
\begin{align}
    \alpha := \mathbb{P}(\theta>0) = \mathbb{E}(\mathbb{1}_{\theta > 0})
\end{align}
The corresponding MonteCarlo estimator, calculated from a posterior random sample of size $n$, is:
\begin{align}
    \hat{\alpha} := \frac{1}{n}\sum_{i=1}^{n}\mathbb{1}_{\theta > 0}(\theta_i) =: \frac{n_+}{n}
\end{align}
where $n_+$ is the number of positive samples.
By direct calculation it is obtained that $\hat{\alpha}$ is an unbiased estimator with variance:
\begin{align}
    \mathbb{E}(\hat{\alpha}) &= \alpha & \mathbb{Var}(\hat{\alpha}) &= \frac{1}{n}\alpha(1-\alpha) 
\end{align}
and therefore we have also the associate plug-in estimator for the variance of the estimator. Let's call it $\hat{\sigma}_{\alpha}$

## Sampling from the posterior distribution: accept-reject algorithm
It is worth noting that in this case there is no need to calculate the normalization constant of the posterior, $Z_g$,
in order to implement the accept-reject algorithm, thanks to a simplification.
```{r echo=TRUE}
# Accept-reject algorithm. K = M Z_g / Z_f, where M is the ratio bound in accept-reject algorithm 
AccRej <- function(n, K){
    samples <- c()
    while(length(samples) < n){
        x <- PriorSample()
        u <- runif(1)
        if(u*K < PosteriorKernel(x) / Proposal(x)){
            samples <- c(samples, x)
        }
  }
  return(samples)
}
```
```{r echo=TRUE}
# Sample from from the posterior using the acc-rej algorithm
x <- seq(-4*sigma,4*sigma,length.out = 100)
y <- PosteriorKernel(x)
treshold <- 1e-4
x_ar <- c()
prob <- c()
varest <- c()
count <- 0
min <- 10

repeat{
    count <- count + 1
    x_ar <- c(x_ar, AccRej(1, K=1))
    prob <- c(prob, length(x_ar[x_ar > 0]) / length(x_ar))
    varest <- c(varest, prob[count] * (1 - prob[count]) / length(x_ar))
    if(varest[count] < treshold && count > min){
        break
    }
}
alpha <- prob[count]
```
```{r echo=FALSE}
# plot
par(mfrow = c(1, 2))
plot(x,y, type="l", xlab=expression(theta), ylab="Posterior kernel", yaxt='n')
hist(x_ar, main="", xlab=expression(theta))
# Restore default plot options
par(mfrow = c(1, 1))
```

The estimated probability using the random sample generated with the accept-reject algorithm is $\mathbb{P}(\theta>0)=$ `r round(alpha,4)`,
and it is calculated with a sample of $n=$ `r length(x_ar)`

The stopping criterion for choosing the sample size was to keep generating random samples until the estimator $\hat{\sigma}_{\alpha}$ went below a pre-determined treshold.

```{r echo=FALSE}
# Plot of the estimator of the variance of the estimator as a function of the sample size
plot(seq(1,count,1),varest, pch="*", cex=0.5, xlab="n", ylab=expression(hat(sigma)[alpha]))
```

Now we can construct a $95\%$ confidence interval as follows:
\begin{align}
    I_{0.95} : \left[ \hat{\alpha}-z_{0.025}\sqrt{\hat{\sigma}_{\alpha} / n} ; \hat{\alpha}+z_{0.025}\sqrt{\hat{\sigma}_{\alpha} / n} \right]
\end{align}
where $z_{1-\alpha}$ are the quantile of the standard normal distribution.
It turns out to be:
```{r echo=FALSE}
print(paste0("Confidence interval [", round(alpha+qnorm(0.025)*sqrt(varest[count]),4), ",", round(alpha+qnorm(0.975)*sqrt(varest[count]),4), "]"))
```

## Sampling from the posterior distribution: Sampling Importance Resampling (SIR) algorithm
The SIR algorithm consists in the following steps:

1. Generate a set of samples from the proposal distribution
2. Calculate the importance weights
3. Normalize the importance weights so that they sum to 1
4. Perform resampling (with replacement) based on the normalized importance weights

```{r echo=TRUE}
# Generate a sample from the proposal distribution
M <- 10000
theta <- PriorSample(M)
# Calculate and normalize weights
w <- PosteriorKernel(theta) / Proposal(theta)
w <- w / length(w)
# Perform resampling
m <- 2000
resample <- function(theta, w) {
  indices <- sample(1:M, size = m, replace = TRUE, prob = w)
  return(theta[indices])
}
theta <- resample(theta, w)
alpha <- length(theta[theta > 0]) / length(theta)
sigma_alpha <- alpha*(1-alpha) / length(theta)
```
```{r echo=FALSE}
# plot
par(mfrow = c(1, 2))
plot(x,y, type="l", xlab=expression(theta), ylab="Posterior kernel", yaxt='n')
hist(theta, xlab=expression(theta), main="")
# Restore default plot options
par(mfrow = c(1, 1))
```

The estimated probability using the random sample generated with the SIR algorithm is $\mathbb{P}(\theta>0)=$ `r round(alpha,4)`

Now we can construct a $95\%$ confidence interval as follows:
```{r echo=FALSE}
print(paste0("Confidence interval [", round(alpha+qnorm(0.025)*sqrt(sigma_alpha),4), ",", round(alpha+qnorm(0.975)*sqrt(sigma_alpha),4), "]"))
```

For the SIR algorithm I used the following references: @10.2307/24307704, @doi:10.1080/01621459.1987.10478461

## Laplace approximation
First of all, we need the mode of the posterior. It is the also the mode of the log posterior $ln g(\theta \vert y)$
```{r echo=TRUE}
laplace <- optim(par=0.0, fn= function (x) log(PosteriorKernel(x)), method = "BFGS",  control=list(fnscale=-1), hessian = TRUE)
# Find the mode
mode <- laplace$par
# Find the variance
var <- -1 / (laplace$hessian)
# Normal approximation
alpha <- pnorm(0, mean=mode, sd=sqrt(var), lower.tail=FALSE)
```
We get the following result:
```{r echo=FALSE}
# Probability that the coin is biased toward heads
print(paste0("Probability that the coin is biased toward heads: ", round(alpha,4)))
```
Justify the stuff about the confidence interval?

<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

#  Exercise 2: Genetic linkage model
  
Suppose $197$ animals are distributed into four categories with the following frequencies

```{r echo=FALSE}
library(knitr)  
x <- 1:4
y <- c(125,18,20,34)  
d <- data.frame(x,y)
names(d) <- c("Category", "Frequency")
kable(d)  
```

Assume that the probabilities of the four categories are given by the vector
  
$$  
\left( \frac{1}{2} + \frac{\theta}{4}, \frac{1}{4} (1 - \theta),  \frac{1}{4} (1 - \theta), \frac{\theta}{4} \right) \ ,
$$
  
where $\theta$ is an unknown parameter between $0$ and $1$. If $\theta$ is assigned a uniform prior, then the posterior density of $\theta$ is given by

$$  
h(\theta  \vert  \text{data}) \propto \left( \frac{1}{2} + \frac{\theta}{4} \right)^{125} \left( \frac{1}{4} (1 - \theta) \right)^{18} \left( \frac{1}{4} (1 - \theta) \right)^{20} \left( \frac{\theta}{4} \right)^{34} \ ,
$$
  
where $0 < \theta < 1$. 

- If $\theta$ is transformed to the real-valued logit $\eta = \log(\theta/(1 - \theta))$, then calculate the posterior density of $\eta$.
- Use a normal approximation to find a $95\%$ probability interval for $\eta$. Transform this interval to obtain a $95\%$ probability interval for the original parameter of interest $\theta$.
- Design an Accept-Reject sampling algorithm for simulating from the posterior density $\eta$. Use a $t$ proposal density using a small number of degrees of freedom and mean and scale parameters given by the normal approximation.
- Compare the results of the two procedures.

## Change of variable 

The posterior density as a function of $\eta$ is obtained by
\begin{align}
    h(\eta \vert \text{data}) \propto h(\theta \vert \text{data})\Bigg|_{\theta = \theta(\eta)} \Bigg| \frac{d\theta}{d\eta} \Bigg|
\end{align}
Using the fact that
\begin{align}
    \theta &= \frac{e^\eta}{1+e^\eta} & \frac{d\theta}{d\eta} &= \frac{d\theta}{d e^{\eta}} \frac{d e^{\eta}}{d\eta}
\end{align}
The calculation yields:
\begin{align}
    h(\eta \vert \text{data}) \propto \frac{(2+3e^\eta)^{125} e^{35\eta}}{(1+e^\eta)^{199}}
\end{align}
The log-posterior is, up to an additive constant:
\begin{align}
    \ln h(\eta) = 125\ln(2+3e^\eta) + 35\eta -199 \ln(1+e^\eta)
\end{align}

```{r echo=TRUE}
# posterior kernel
PosteriorKernel <- function(eta){
    ker <- (2 + 3*exp(eta))^(125) * exp(35*eta) / (1 + exp(eta))^(199)
    return (ker)
}  
```
```{r echo=FALSE}
# plot
x <- seq(-1, 2, length.out = 100)
y <- PosteriorKernel(x)
plot(x,y, type="l", xlab=expression(eta), ylab="Posterior kernel", yaxt='n')
```

## Laplace approximation
```{r echo=TRUE}
laplace <- optim(par=0.0, fn= function (x) log(PosteriorKernel(x)), method = "BFGS",  control=list(fnscale=-1), hessian = TRUE)
# Find the mode
mode <- laplace$par
# Find the variance
var <- -1 / (laplace$hessian)
```
A $95\%$ confidence interval for $\eta$ is
```{r echo=FALSE}
print(paste0("Confidence interval [", round(mode+qnorm(0.025)*sqrt(var),4), ",", round(mode+qnorm(0.975)*sqrt(var),4), "]"))
```
In order to find a $95\%$ confidence interval for $\theta$ we need to perform the change of variable
```{r echo=TRUE}

```

## Accept-Reject algorithm using a t-distribution as a proposal
```{r echo=TRUE, warning=FALSE}  
library(mnormt)
dof = 3
# Proposal distrubution
Proposal <- function(x, df = dof, mu = mode, s = sqrt(var)){
  return(dmt(x, mean=mu, S=s, df = df))
}
# Sample from proposal
ProposalSample <- function(n=1){
    x <- rmt(n, mode, sqrt(var), dof)
    return (x)
}
```

In the following the proposal distribution and a sample from it is shown (inserire label)
```{r echo=FALSE, warning=FALSE}
# plot
x <- seq(-2, 2, length.out = 100)
y <- Proposal(x)
par(mfrow = c(1, 2))
plot(x,y, type="l", ylab="Proposal t")
sample <- ProposalSample(10000)
hist(sample, main="")
# Restore default plot options
par(mfrow = c(1, 1))
```

Now sample from the posterior $h(\eta)$ using the accept-reject algorithm, in a similar way as did in Exercise 1,
but in this case 
```{r echo=TRUE}
# Sample from from the posterior using the acc-rej algorithm
treshold <- 1e-4
x_ar <- c()
varest <- c()
count <- 0
min <- 10

repeat{
    count <- count + 1
    x_ar <- c(x_ar, AccRej(1, K=1))
    prob <- c(prob, length(x_ar[x_ar > 0]) / length(x_ar))
    varest <- c(varest, prob[count] * (1 - prob[count]) / length(x_ar))
    if(varest[count] < treshold && count > min){
        break
    }
}
alpha <- prob[count]
```


<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# Exercise 3: Poisson Regression

Consider an experiment involving subjects reporting one stressful event. The collected data $y_{1}, \cdots, y_{18}$ where $y_{i}$ is the number of events recalled $i$ months before the interview. Suppose $y_i$ is Poisson distributed with mean $\lambda_i$, where the $\lambda_i$s satisfy the loglinear regression model

$$
\log \lambda_i = \beta_0 +\beta_1 i \ .
$$    

The data are shown in the following table

```{r echo=FALSE}  
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18)
y <- c(15, 11, 14, 17, 5, 11, 10, 4, 8, 10, 7, 9, 11, 3, 6, 1, 1, 4)
d <- data.frame(x,y)
names <- c("Months", "$y_i$")
kable(d)
```
  
If $(\beta_0, \beta_1)$ is assigned a uniform prior, then the log of the posterior density is given, up to an additive constant, by

$$
\log(g(\beta_0, \beta_1  \vert  data)) \propto \sum_{i=1}^{18} \left[ y_i (\beta_0 + \beta_1 i ) - \exp(\beta_0 + \beta_1 i) \right]
$$     

- Write an R function to compare the log of the posterior density of $(\beta_0, \beta_1)$.
- Suppose we are interested in estimating the posterior mean and standard deviation for the slope $\beta_1$. Approximate these moments by a normal approximation about the posterior mode.
- Use a multivariate $t$ proposal density and the SIR algorithm to simulate 1000 draws from the posterior density. Use this sample to estimate the posterior mean and standard deviation of the slope $\beta_1$.
- Compare your estimates with the estimates using the normal approximation.


<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# Exercise 4:  Variance components model

Consider the data concerning batch-to-batch variation in yields of dyestuff. The following data arise from a balanced experiment whereby the total product yield was determined for five samples from each of six randomly chosen batches of raw material.

```{r echo=FALSE}  
x <- c(1545, 1440, 1440, 1520, 1580, 1540, 1555, 1490, 1560, 1495, 1595, 1550, 1605, 1510, 1560, 1445, 1440, 1595, 1465, 1545, 1595, 1630, 1515, 1635, 1625, 1520, 1455, 1450, 1480, 1445)
x <- matrix(x, ncol=5, byrow=TRUE)
dimnames(x) <- list(1:6, paste0("S", 1:5))  
kable(x, row.names=TRUE)      
```

Let $y_{ij}$ denote the $j$th observation in batch $i$. To determine the relative importance of between-batch variation versus sampling variation, the following multilevel model is applied ($N$ denotes the number of batches and $n$ denotes the number of observations per batch).

- $y_{ij} \sim N(\mu + b_i, \sigma_y)$, $i=1,\ldots,N$, $j=1,\ldots,n$.
- $b_i \sim N(0, \sigma_b)$, $i=1,\ldots,N$.
- $(\sigma^2_y, \sigma^2_b)$ is assigned a uniform prior.

In this situation, the focus is on the marginal posterior distribution of the variance components. It is possible to analytically integrate out the random effects $b_i$s, resulting in the marginal posterior density of $(\mu, \sigma^2_y, \sigma^2_b)$ given, up to a proportionally constant, by

$$
\prod_{i=1}^N \left[ \phi\left( \bar{y}_i  \vert  \mu, \sqrt{\sigma^2_y/n + \sigma^2_b} \right) f_G\left( S_i  \vert  (n-1)/2, 1/(2 \sigma^2_y) \right) \right] \ , 
$$

where $\hat{y}_i$ and $S_i$ are respectively the mean yield and the "within sum of squares" of the $i$th batch, $\phi\left(y  \vert  \mu,\sigma\right)$ is the normal density of mean $\mu$ and standard deviation $\sigma$, and $f_G(y  \vert  a,b)$ is the gamma density proportional to $y^{a-1} \exp( - b y)$.

- Write an R function for the log of the posterior density with parametrization $\theta = (\mu, \log\sigma_y, \log\sigma_b)$.
- Using a normal approximation, to this aim,
   - find the posterior mode of $\theta$ using a numerical method and try the following alternative starting values
     - $\theta=(1500, 3, 3)$
     - $\theta=(1500, 1, 1)$
     - $\theta=(1500, 10, 10)$
  
     and assess the sensitivity of the numerical method to the starting value.
- Use the normal approximation to find $90\%$ interval estimates for the log of the standard deviation $\log\sigma_y$, $\log\sigma_b$.
- Using the results from the previous point find $90\%$ interval estimates for the variance components $\sigma_y^2$, $\sigma_b^2$.


<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# References
