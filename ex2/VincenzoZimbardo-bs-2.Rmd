---
title: Homework 1 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
---

# Exercise 1: Estimating a log-odds with a normal prior

Suppose $Y$ has a binomial distribution with parameters $n$ and $p$, and we are interesting in the log-odds value $\theta=\operatorname{logit}(p) = \log(p/(1-p))$. Our prior for $\theta$ is that $\theta \sim N(\mu, \sigma)$ (parametrized as mean and standard deviation). It follows that the posterior density of $\theta$ is given, up to a proportional constant, by

$$
g(\theta \vert y) \propto \frac{\exp(y\theta)}{(1+\exp(\theta))^n} \exp\left[ -\frac{(\theta-\mu)^2}{2 \sigma^2} \right]
$$  

More concretely, suppose we are interested in learning about the probability that a special coin lands heads when tossed. A priori we believe that the coin is fair, so we assign $\theta$ an $N(0, .25)$ prior. We toss the coin $n=5$ times and obtain $y=5$ heads.

- Using the prior density as a proposal density, design an Accept-Reject algorithm for sampling from the posterior distribution. Using simulated draws from your algorithm, approximate the probability that the coin is biased toward heads.

- Using the prior density as a proposal density, simulate from the posterior distribution using a Sampling Importance Resampling (SIR) algorithm. Approximate the probability that the coin is biased toward heads.
- Use a Laplace approximation to estimate the probability that the coin is biased toward heads. 
- In all above cases construct a $95\%$ probability interval.


```{r echo=TRUE}  
# Parameters
mu <- 0
sigma <- 0.25
n_data <- 5
y_data <- 5
```
```{r echo=TRUE}  
# Posterior kernel
PosteriorKernel <- function(theta){
    ker <- exp(y_data*theta) / (1+exp(theta))^n_data
    ker <- ker * exp(-(theta-mu)^2/(2*sigma^2))
    return (ker)
}  
# Prior kernel
PriorKernel <- function(theta){
    ker <- exp(-(theta-mu)^2/(2*sigma^2))
    return (ker)
}
# Prior normalization
Z <- sqrt(2*pi*sigma^2)
# Sample from prior
PriorSample <- function(){
    x <- rnorm(1,mu,sigma)
    return (x)
}
```
The aim is to estimate the probability that the coin is biased toward head. This means that we want to estimate the following quantity:
\begin{align}
    \alpha := \mathbb{P}(\theta>0) = \mathbb{E}(\mathbb{1}_{\theta > 0})
\end{align}
The corresponding MonteCarlo estimator, calculated from a posterior random sample of size $n$, is:
\begin{align}
    \hat{\alpha} := \frac{1}{n}\sum_{i=1}^{n}\mathbb{1}_{\theta > 0}(\theta_i) =: \frac{n_+}{n}
\end{align}
where $n_+$ is the number of positive samples.
By direct calculation it is obtained that $\hat{\alpha}$ is an unbiased estimator with variance:
\begin{align}
    \mathbb{E}(\hat{\alpha}) &= \alpha & \mathbb{Var}(\hat{\alpha}) &= \frac{1}{n}\alpha(1-\alpha) 
\end{align}
and therefore we have also the associate plug-in estimator for the variance of the estimator. Let's call it $\hat{\sigma}_{\alpha}$

## Sampling from the posterior distribution: accept-reject algorithm
```{r echo=TRUE}
# Accept-reject algorithm. K = M Z_g / Z_f, where M is the ratio bound in accept-reject algorithm 
AccRej <- function(n, K){
    samples <- c()
    while(length(samples) < n){
        x <- PriorSample()
        u <- runif(1)
        if(u*K < PosteriorKernel(x) / PriorKernel(x)){
            samples <- c(samples, x)
        }
  }
  return(samples)
}
```
```{r echo=TRUE}
# Sample from from the posterior using the acc-rej algorithm
x <- seq(-4*sigma,4*sigma,length.out = 100)
y <- PosteriorKernel(x)
treshold <- 1e-4
x_ar <- c()
prob <- c()
varest <- c()
count <- 0
min <- 10

repeat{
    count <- count + 1
    x_ar <- c(x_ar, AccRej(1, K=1))
    prob <- c(prob, length(x_ar[x_ar > 0]) / length(x_ar))
    varest <- c(varest, prob[count] * (1 - prob[count]) / length(x_ar))
    if(varest[count] < treshold && count > min){
        break
    }
}
alpha <- prob[count]

# plot
par(mfrow = c(1, 2))
plot(x,y, type="l")
hist(x_ar)
```
```{r echo=FALSE}
par(mfrow = c(1, 1))
```

The estimated probability using the random sample generated with the accept-reject algorithm is $\mathbb{P}(\theta>0)=$ `r alpha`,
and it is calculated with a sample of $n=$ `r length(x_ar)`

Add how n was choosed

```{r echo=FALSE}
plot(seq(1,count,1),varest, pch="*", cex=0.5)
```

Now we can construct a $95\%$ confidence interval as follows:
\begin{align}
    I_{0.95} : \left[ \hat{\alpha}-z_{0.025}\sqrt{\hat{\sigma}_{\alpha} / n} ; \hat{\alpha}+z_{0.025}\sqrt{\hat{\sigma}_{\alpha} / n} \right]
\end{align}
It turn out to be:
```{r echo=TRUE}
print(paste0("Confidence interval of phi: [",round(alpha+qnorm(0.025)*sqrt(varest[count]),4), ", ", round(alpha+qnorm(0.975)*sqrt(varest[count]),4), "]"))
```

## Sampling from the posterior distribution: Sampling Importance Resampling (SIR) algorithm

#  Exercise 2: Genetic linkage model
  
Suppose $197$ animals are distributed into four categories with the following frequencies

```{r echo=FALSE}
library(knitr)  
x <- 1:4
y <- c(125,18,20,34)  
d <- data.frame(x,y)
names(d) <- c("Category", "Frequency")
kable(d)  
```

Assume that the probabilities of the four categories are given by the vector
  
$$  
\left( \frac{1}{2} + \frac{\theta}{4}, \frac{1}{4} (1 - \theta),  \frac{1}{4} (1 - \theta), \frac{\theta}{4} \right) \ ,
$$
  
where $\theta$ is an unknown parameter between $0$ and $1$. If $\theta$ is assigned a uniform prior, then the posterior density of $\theta$ is given by

$$  
h(\theta  \vert  \text{data}) \propto \left( \frac{1}{2} + \frac{\theta}{4} \right)^{125} \left( \frac{1}{4} (1 - \theta) \right)^{18} \left( \frac{1}{4} (1 - \theta) \right)^{20} \left( \frac{\theta}{4} \right)^{34} \ ,
$$
  
where $0 < \theta < 1$. 

- If $\theta$ is transformed to the real-valued logit $\eta = \log(\theta/(1 - \theta))$, then calculate the posterior density of $\eta$.
- Use a normal approximation to find a $95\%$ probability interval for $\eta$. Transform this interval to obtain a $95\%$ probability interval for the original parameter of interest $\theta$.
- Design an Accept-Reject sampling algorithm for simulating from the posterior density $\eta$. Use a $t$ proposal density using a small number of degrees of freedom and mean and scale parameters given by the normal approximation.
- Compare the results of the two procedures.

# Exercise 3: Poisson Regression

Consider an experiment involving subjects reporting one stressful event. The collected data $y_{1}, \cdots, y_{18}$ where $y_{i}$ is the number of events recalled $i$ months before the interview. Suppose $y_i$ is Poisson distributed with mean $\lambda_i$, where the $\lambda_i$s satisfy the loglinear regression model

$$
\log \lambda_i = \beta_0 +\beta_1 i \ .
$$    

The data are shown in the following table

```{r echo=FALSE}  
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18)
y <- c(15, 11, 14, 17, 5, 11, 10, 4, 8, 10, 7, 9, 11, 3, 6, 1, 1, 4)
d <- data.frame(x,y)
names <- c("Months", "$y_i$")
kable(d)
```
  
If $(\beta_0, \beta_1)$ is assigned a uniform prior, then the log of the posterior density is given, up to an additive constant, by

$$
\log(g(\beta_0, \beta_1  \vert  data)) \propto \sum_{i=1}^{18} \left[ y_i (\beta_0 + \beta_1 i ) - \exp(\beta_0 + \beta_1 i) \right]
$$     

- Write an R function to compare the log of the posterior density of $(\beta_0, \beta_1)$.
- Suppose we are interested in estimating the posterior mean and standard deviation for the slope $\beta_1$. Approximate these moments by a normal approximation about the posterior mode.
- Use a multivariate $t$ proposal density and the SIR algorithm to simulate 1000 draws from the posterior density. Use this sample to estimate the posterior mean and standard deviation of the slope $\beta_1$.
- Compare your estimates with the estimates using the normal approximation.