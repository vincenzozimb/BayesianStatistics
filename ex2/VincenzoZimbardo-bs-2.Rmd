---
title: Homework 2 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
bibliography: references.bib
---

# Exercise 1: Estimating a log-odds with a normal prior

Suppose $Y$ has a binomial distribution with parameters $n$ and $p$, and we are interesting in the log-odds value $\theta=\operatorname{logit}(p) = \log(p/(1-p))$. Our prior for $\theta$ is that $\theta \sim N(\mu, \sigma)$ (parametrized as mean and standard deviation). It follows that the posterior density of $\theta$ is given, up to a proportional constant, by

$$
g(\theta \vert y) \propto \frac{\exp(y\theta)}{(1+\exp(\theta))^n} \exp\left[ -\frac{(\theta-\mu)^2}{2 \sigma^2} \right]
$$  

More concretely, suppose we are interested in learning about the probability that a special coin lands heads when tossed. A priori we believe that the coin is fair, so we assign $\theta$ an $N(0, .25)$ prior. We toss the coin $n=5$ times and obtain $y=5$ heads.

- Using the prior density as a proposal density, design an Accept-Reject algorithm for sampling from the posterior distribution. Using simulated draws from your algorithm, approximate the probability that the coin is biased toward heads.

- Using the prior density as a proposal density, simulate from the posterior distribution using a Sampling Importance Resampling (SIR) algorithm. Approximate the probability that the coin is biased toward heads.
- Use a Laplace approximation to estimate the probability that the coin is biased toward heads. 
- In all above cases construct a $95\%$ probability interval.


```{r echo=FALSE}  
# Initial seed
set.seed(12345)
```
```{r echo=TRUE}  
# Parameters
mu <- 0
sigma <- 0.25
n_data <- 5
y_data <- 5
```
```{r echo=TRUE}  
# Posterior kernel
PosteriorKernel <- function(theta){
    ker <- exp(y_data*theta) / (1+exp(theta))^n_data
    ker <- ker * exp(-(theta-mu)^2/(2*sigma^2))
    return (ker)
}  
# Prior kernel as a proposal distribution
ProposalKernel <- function(theta){
    ker <- exp(-(theta-mu)^2/(2*sigma^2))
    return (ker)
}
# Prior normalization
Z <- sqrt(2*pi*sigma^2)
# Sample from proposal
ProposalSample <- function(n=1){
    x <- rnorm(n,mu,sigma)
    return (x)
}
```
The aim is to estimate the probability that the coin is biased toward head. This means that we want to estimate the following quantity:
\begin{align}
    \alpha := \mathbb{P}(\theta>0) = \mathbb{E}(\mathbb{1}_{\theta > 0})
\end{align}
The corresponding MonteCarlo estimator, calculated from a posterior random sample of size $n$, is:
\begin{align}
    \hat{\alpha} := \frac{1}{n}\sum_{i=1}^{n}\mathbb{1}_{\theta > 0}(\theta_i) =: \frac{n_+}{n}
\end{align}
where $n_+$ is the number of positive samples.
By direct calculation it is obtained that $\hat{\alpha}$ is an unbiased estimator with variance:
\begin{align}
    \mathbb{E}(\hat{\alpha}) &= \alpha & \mathbb{Var}(\hat{\alpha}) &= \frac{1}{n}\alpha(1-\alpha) 
\end{align}
and therefore we have also the associate plug-in estimator for the variance of the estimator. Let's call it $\hat{\sigma}_{\alpha}$

## Sampling from the posterior distribution: accept-reject algorithm
It is worth noting that in this case there is no need to calculate the normalization constant of the posterior, $Z_g$,
in order to implement the accept-reject algorithm, thanks to a simplification.
```{r echo=TRUE}
# Accept-reject algorithm. K = M * Z_target / Z_proposal, where M is the ratio bound in accept-reject algorithm.
AccRej <- function(n, K){
    samples <- c()
    while(length(samples) < n){
        x <- ProposalSample()
        u <- runif(1)
        if(u*K < PosteriorKernel(x) / ProposalKernel(x)){
            samples <- c(samples, x)
        }
  }
  return(samples)
}
```
```{r echo=TRUE}
# Sample from from the posterior using the acc-rej algorithm
x <- seq(-4*sigma,4*sigma,length.out = 100)
y <- PosteriorKernel(x)
threshold <- 1e-5
x_ar <- c()
prob <- c()
varest <- c()
count <- 0
min <- 10
max <- 40000

repeat{
    count <- count + 1
    x_ar <- c(x_ar, AccRej(1, K=1))
    prob <- c(prob, length(x_ar[x_ar > 0]) / length(x_ar))
    varest <- c(varest, prob[count] * (1 - prob[count]) / length(x_ar))
    if( ( varest[count] < threshold && count > min ) || count >= max ){
        break
    }
}
alpha <- prob[count]
```
```{r echo=FALSE}
# plot
par(mfrow = c(1, 2))
plot(x,y, type="l", xlab=expression(theta), ylab="Posterior kernel", yaxt='n')
hist(x_ar, main="", xlab=expression(theta))
# Restore default plot options
par(mfrow = c(1, 1))
```

The estimated probability using the random sample generated with the accept-reject algorithm is $\mathbb{P}(\theta>0)=$ `r round(alpha,4)`,
and it is calculated with a sample of $n=$ `r length(x_ar)`

The stopping criterion for choosing the sample size was to keep generating random samples until the estimator $\hat{\sigma}_{\alpha}$ went below a pre-determined threshold.

```{r echo=FALSE}
# Plot of the estimator of the variance of the estimator as a function of the sample size
plot(seq(1,count,1),varest, pch="*", cex=0.5, xlab="n", ylab=expression(hat(sigma)[alpha]))
```

Now we can construct a $95\%$ confidence interval as follows:
\begin{align}
    I_{0.95} : \left[ \hat{\alpha}-z_{0.025}\sqrt{\hat{\sigma}_{\alpha} / n} ; \hat{\alpha}+z_{0.025}\sqrt{\hat{\sigma}_{\alpha} / n} \right]
\end{align}
where $z_{1-\alpha}$ are the quantile of the standard normal distribution.
It turns out to be:
```{r echo=FALSE}
print(paste0("Confidence interval [", round(alpha+qnorm(0.025)*sqrt(varest[count]),4), ",", round(alpha+qnorm(0.975)*sqrt(varest[count]),4), "]"))
```

## Sampling from the posterior distribution: Sampling Importance Resampling (SIR) algorithm
The SIR algorithm consists in the following steps:

1. Generate a set of samples from the proposal distribution
2. Calculate the importance weights
3. Normalize the importance weights so that they sum to 1
4. Perform resampling (with replacement) based on the normalized importance weights

```{r echo=TRUE}
# Generate a sample from the proposal distribution
M <- 10000
theta <- ProposalSample(M)
# Calculate and normalize weights
w <- PosteriorKernel(theta) / ProposalKernel(theta)
w <- w / sum(w)
# Perform resampling
m <- 2000
resample <- function(theta, w) {
  indices <- sample(1:M, size = m, replace = TRUE, prob = w)
  return(theta[indices])
}
theta <- resample(theta, w)
alpha <- length(theta[theta > 0]) / length(theta)
sigma_alpha <- alpha*(1-alpha) / length(theta)
```
```{r echo=FALSE}
# plot
par(mfrow = c(1, 2))
plot(x,y, type="l", xlab=expression(theta), ylab="Posterior kernel", yaxt='n')
hist(theta, xlab=expression(theta), main="")
# Restore default plot options
par(mfrow = c(1, 1))
```

The estimated probability using the random sample generated with the SIR algorithm is $\mathbb{P}(\theta>0)=$ `r round(alpha,4)`

Now we can construct a $95\%$ confidence interval as follows:
```{r echo=FALSE}
print(paste0("Confidence interval [", round(alpha+qnorm(0.025)*sqrt(sigma_alpha),4), ",", round(alpha+qnorm(0.975)*sqrt(sigma_alpha),4), "]"))
```

For the SIR algorithm I used the following references: @10.2307/24307704, @doi:10.1080/01621459.1987.10478461

## Laplace approximation
First of all, we need the mode of the posterior. It is the also the mode of the log posterior $\log g(\theta \vert y)$
```{r echo=TRUE}
laplace <- optim(par=0.0, fn= function (x) log(PosteriorKernel(x)), method = "BFGS",  control=list(fnscale=-1), hessian = TRUE)
# Find the mode
mode <- laplace$par
# Find the variance
var <- -1 / (laplace$hessian)
# Normal approximation
alpha <- pnorm(0, mean=mode, sd=sqrt(var), lower.tail=FALSE)
```
We get the following result:
```{r echo=FALSE}
# Probability that the coin is biased toward heads
print(paste0("Probability that the coin is biased toward heads: ", round(alpha,4)))
```

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

#  Exercise 2: Genetic linkage model
  
Suppose $197$ animals are distributed into four categories with the following frequencies

```{r echo=FALSE}
library(knitr)  
x <- 1:4
y <- c(125,18,20,34)  
d <- data.frame(x,y)
names(d) <- c("Category", "Frequency")
kable(d)
```

Assume that the probabilities of the four categories are given by the vector
  
$$  
\left( \frac{1}{2} + \frac{\theta}{4}, \frac{1}{4} (1 - \theta),  \frac{1}{4} (1 - \theta), \frac{\theta}{4} \right) \ ,
$$
  
where $\theta$ is an unknown parameter between $0$ and $1$. If $\theta$ is assigned a uniform prior, then the posterior density of $\theta$ is given by

$$  
h(\theta  \vert  \text{data}) \propto \left( \frac{1}{2} + \frac{\theta}{4} \right)^{125} \left( \frac{1}{4} (1 - \theta) \right)^{18} \left( \frac{1}{4} (1 - \theta) \right)^{20} \left( \frac{\theta}{4} \right)^{34} \ ,
$$
  
where $0 < \theta < 1$. 

- If $\theta$ is transformed to the real-valued logit $\eta = \log(\theta/(1 - \theta))$, then calculate the posterior density of $\eta$.
- Use a normal approximation to find a $95\%$ probability interval for $\eta$. Transform this interval to obtain a $95\%$ probability interval for the original parameter of interest $\theta$.
- Design an Accept-Reject sampling algorithm for simulating from the posterior density $\eta$. Use a $t$ proposal density using a small number of degrees of freedom and mean and scale parameters given by the normal approximation.
- Compare the results of the two procedures.

## Change of variable 

The posterior density as a function of $\eta$ is obtained by
\begin{align}
    h(\eta \vert \text{data}) \propto h(\theta \vert \text{data})\Bigg|_{\theta = \theta(\eta)} \Bigg| \frac{d\theta}{d\eta} \Bigg|
\end{align}
Using the fact that
\begin{align}
    \theta &= \frac{e^\eta}{1+e^\eta} & \frac{d\theta}{d\eta} &= \frac{d\theta}{d e^{\eta}} \frac{d e^{\eta}}{d\eta}
\end{align}
The calculation yields:
\begin{align}
    h(\eta \vert \text{data}) \propto \frac{(2+3e^\eta)^{125} e^{35\eta}}{(1+e^\eta)^{199}}
\end{align}
The log-posterior is, up to an additive constant:
\begin{align}
    \ln h(\eta \vert \text{data}) = 125\ln(2+3e^\eta) + 35\eta -199 \ln(1+e^\eta)
\end{align}

```{r echo=TRUE}
# Posterior kernel
PosteriorKernel <- function(eta){
    ker <- 125*log(2+3*exp(eta)) + 35*eta -199*log(1+exp(eta)) - 197*log(4)
    return (exp(ker))
}
# Posterior normalization
Z_post <- integrate(function(x) (PosteriorKernel(x)), -Inf, Inf)$value
# Posterior density
Posterior <- function(x){
    return ( PosteriorKernel(x) / Z_post )
}  
```

## Laplace approximation
```{r echo=TRUE}
laplace <- optim(par=0.0, fn= function (x) log(PosteriorKernel(x)), method = "BFGS",  control=list(fnscale=-1), hessian = TRUE)
# Find the mode
mode <- laplace$par
# Find the variance
var <- -1 / (laplace$hessian)
```

```{r echo=FALSE}
# plot
x <- seq(-2, 2, length.out = 100)
y <- Posterior(x)
plot(x,y, type="l", lwd="1", col="black", xlab=expression(eta), ylab="Posterior density")
y <- dnorm(x,mode,sqrt(var))
points(x,y, col="red")
legend( "topleft", legend=c( "Posterior density", "Laplace approximation"), col=c("black", "red"), lty=c(1,2))
```

The estimated mean value and standard deviation from the Poisson approximation are
```{r echo=FALSE}
print(paste0("Mean value: ", round(mode,4)))
print(paste0("Standard deviation: ", round(sqrt(var),4)))
```

A $95\%$ confidence interval for $\eta$ is
```{r echo=FALSE}
low_eta <- round(mode+qnorm(0.025)*sqrt(var),4)
high_eta <- round(mode+qnorm(0.975)*sqrt(var),4)
CI_la <- c("Confidence interval", "[", low_eta, ",", high_eta, "]")
print(paste0(CI_la, collapse=""))
CI_la <- CI_la[-1]
```

In order to find a $95\%$ confidence interval for $\theta$ we need to perform the change of variable
```{r echo=FALSE}
low_theta <- round(exp(low_eta) / (1 + exp(low_eta)),4)
high_theta <- round(exp(high_eta) / (1 + exp(high_eta)),4)
CI_la_theta <- c("Confidence interval", "[", low_theta, ",", high_theta, "]")
print(paste0(CI_la_theta, collapse=""))
```

## Accept-Reject algorithm using a t-distribution as a proposal
```{r echo=TRUE, warning=FALSE}  
library(mnormt)
dof = 3
# Proposal distrubution
Proposal <- function(x){
  return(dmt(x, mean=mode, S=sqrt(var), df = dof))
}
# Sample from proposal
ProposalSample <- function(n=1){
    x <- rmt(n, mode, sqrt(var), dof)
    return (x)
}
```

Now sample from the posterior $h(\eta)$ using the accept-reject algorithm, in a similar way as did in Exercise 1,
but in this case we need to calculate the ratio bound
```{r echo=TRUE}
# Sample from from the posterior using the acc-rej algorithm.
optim <- optim(par=0.0, fn= function (x) Posterior(x) / Proposal(x), method = "BFGS",  control=list(fnscale=-1))
M <- optim$value
print(paste0(c("M=", round(M,4)), collapse=""))
```

```{r echo=FALSE, warning=FALSE}
# plot
x <- seq(-2, 2, length.out = 100)
y <- M * dmt(x,mode,sqrt(var),dof)
plot(x,y, type="l", ylim=c(0,max(y)), xlab=expression(eta), ylab="Densities")
y <- Posterior(x)
lines(x,y, col="red", ylab="Posterior density")
legend( "topleft", legend=c( "M * Proposal", "Posterior density"), col=c("black", "red"), lty=c(1,1))
```

```{r echo=TRUE}
# Accept-reject algorithm
AccRej <- function(n, M){
    samples <- c()
    while(length(samples) < n){
        x <- ProposalSample()
        u <- runif(1)
        if(u*M < Posterior(x) / Proposal(x)){
            samples <- c(samples, x)
        }
  }
  return(samples)
}
```

```{r echo=TRUE}
threshold <- 1e-8
x_ar <- c()
varest <- c()
count <- 0
min <- 10

repeat{
    count <- count + 1
    x_ar <- c(x_ar, AccRej(1, M))
    varest <- c(varest, sd(x_ar))
    if(abs(varest[count]-varest[count-1]) < threshold && count > min){
        break
    }
}

x_ar <- x_ar[-1]
varest <- varest[-1]
count <- count -1

mean <- mean(x_ar)
sdv <- varest[count]
```

```{r echo=FALSE}
# plot
x <- seq(-0.5, 1.5, length.out = 100)
y <- Posterior(x)
hist(x_ar, main="", xlab="sample", ylab="Posterior density", freq=FALSE)
lines(x,y, type="l")
```

The estimated mean value and standard deviation from the sample (of dimension $n=$ `r length(x_ar)`) are

```{r echo=FALSE}
print(paste0("Mean value: ", round(mean,4)))
print(paste0("Standard deviation: ", round(sdv,4)))
```

The stopping criterion for choosing the sample size was to keep generating random samples until the sample standard deviation 
stabilizes around a limiting value, up to variations below a predetermined threshold.

A $95\%$ confidence interval is: 
```{r echo=FALSE}
CI_ac <- c("Confidence interval", "[", round(mean+qnorm(0.025)*sdv,4), ",", round(mean+qnorm(0.975)*sdv,4), "]")
print(paste0(CI_ac, collapse=""))
CI_ac <- CI_ac[-1]
```

## Comparison of the two methods

Comparison for $\eta$ estimation in the following table

| Parameter           | Accept-Reject sampling               |Normal approximation                  |
|---------------------|--------------------------------------|--------------------------------------|
| mean                | `r mean`                             | `r mode`                             |
| standard deviation  | `r sdv`                              | `r sqrt(var)`                        |
| confidence interval | `r paste0(CI_ac, collapse="")`       | `r paste0(CI_la, collapse="")`       |

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# Exercise 3: Poisson Regression

Consider an experiment involving subjects reporting one stressful event. The collected data $y_{1}, \cdots, y_{18}$ where $y_{i}$ is the number of events recalled $i$ months before the interview. Suppose $y_i$ is Poisson distributed with mean $\lambda_i$, where the $\lambda_i$s satisfy the loglinear regression model

$$
\log \lambda_i = \beta_0 +\beta_1 i \ .
$$    

The data are shown in the following table

```{r echo=FALSE}  
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18)
y <- c(15, 11, 14, 17, 5, 11, 10, 4, 8, 10, 7, 9, 11, 3, 6, 1, 1, 4)
d <- data.frame(x,y)
names <- c("Months", "$y_i$")
kable(d)
```
  
If $(\beta_0, \beta_1)$ is assigned a uniform prior, then the log of the posterior density is given, up to an additive constant, by

$$
\log(g(\beta_0, \beta_1  \vert  data)) \propto \sum_{i=1}^{18} \left[ y_i (\beta_0 + \beta_1 i ) - \exp(\beta_0 + \beta_1 i) \right]
$$     

- Write an R function to compute the log of the posterior density of $(\beta_0, \beta_1)$.
- Suppose we are interested in estimating the posterior mean and standard deviation for the slope $\beta_1$. Approximate these moments by a normal approximation about the posterior mode.
- Use a multivariate $t$ proposal density and the SIR algorithm to simulate 1000 draws from the posterior density. Use this sample to estimate the posterior mean and standard deviation of the slope $\beta_1$.
- Compare your estimates with the estimates using the normal approximation.


## Write an R function to compute the posterior log density

It is useful to define, given $N=$ `r length(x)`:
\begin{align}
    A &:= \sum_{i=1}^{N} y_i & D &:= \sum_{i=1}^{N} i~y_i
\end{align}
Moreover:
\begin{equation}
    f(\beta_1) := \sum_{i=1}^{N} e^{\beta_1 i}
\end{equation}
In this way we can rewrite, up to an additive constant:
\begin{equation}
    \log(g(\beta_0, \beta_1 \vert data)) = A\beta_0 + D\beta_1 - f(\beta_1)e^{\beta_0}
\end{equation}
And consequently:
\begin{equation}
    g(\beta_0, \beta_1 \vert data) \propto e^{A\beta_0} e^{D\beta_1} e^{-f(\beta_1) e^{\beta_0}}
\end{equation}


```{r echo=TRUE}
# Constants
N <- length(x)
A <- sum(y)
D <- sum(y * seq_along(y))
# Auxiliary function
func <- function(beta1){
    res <- sum(exp(beta1*x))
    return (res)
}
# log posterior, up to the additive constant (related to the normalization)
logPost <- function(beta){
    beta0 <- beta[1]
    beta1 <- beta[2]
    res <- A*beta0 + D*beta1 - func(beta1)*exp(beta0)
    return (res)
}
# Posterior kernel
PosteriorKernel <- function(beta){
    return (exp(logPost(beta)))
}
```

## Laplace's approximation

Find the mode in order to procede with the Laplace's approximation

```{r echo=TRUE}
laplace <- optim(par=c(0, -0.5), fn = logPost, control=list(fnscale=-1), method = "Nelder-Mead", hessian = TRUE)
# Find the mode
mode <- laplace$par
# Find the variance
Sigma <- -solve(laplace$hessian)
var <- Sigma[2,2]
```
The mean and the standard deviation are then:
```{r echo=FALSE}
print(paste0("Mean value: ", round(mode[2],4)))
print(paste0("Standard deviation: ", round(sqrt(var),4)))
```

## SIR algorithm with t proposal density

```{r echo=TRUE}
library(mvtnorm)
dof <- 5
# Proposal density. A multivariate t (parameters given by the Laplace's approximation)
Proposal <- function(x){
    return (dmvt(x, delta=mode, sigma=Sigma, df=dof, type="shifted", log=FALSE))
}
ProposalSample <- function(n=1){
    return (rmvt(n=n, sigma=Sigma, df=dof, delta=mode, type="shifted"))
}
```

```{r echo=TRUE}
# Generate a sample from the proposal distribution
m <- 1000
M <- m * 10
betasample <- ProposalSample(M)
# Calculate and normalize weights
weights <- function(betasample){
    numer <- apply(betasample, 1, PosteriorKernel)
    denom <- apply(betasample, 1, Proposal)
    w <- numer / denom
    w <- w / sum(w)
    return (w)
}
w <- weights(betasample)
# Perform resampling
resample <- function(betasample, w){
  indices <- sample(1:M, size = m, replace = TRUE, prob = w)
  return(betasample[indices,])
}
betasample <- resample(betasample, w)
```
Now that we can use this random sample to calculate the mean value and the standard deviation of $\beta_1$.
We get:
```{r echo=FALSE}
print(paste0("Mean value: ", round(mean(betasample[,2]),4)))
print(paste0("Standard deviation: ", round(sd(betasample[,2]),4)))
```

## Comparison

Comparison of the two methods in the following table

| Parameter           | Normal approximation    | SIR algorithm                        |
|---------------------|-------------------------|--------------------------------------|
| mean                | `r round(mode[2],4)`    | `r round(mean(betasample[,2]),4)`    |
| standard deviation  | `r round(sqrt(var),4)`  | `r round(sd(betasample[,2]),4)`      |

We conclude that the two approach give very similar results.

```{r echo=FALSE}
# Clear workspace exept specified objects
rm(list=ls())
```
<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# Exercise 4:  Variance components model

Consider the data concerning batch-to-batch variation in yields of dyestuff. The following data arise from a balanced experiment whereby the total product yield was determined for five samples from each of six randomly chosen batches of raw material.

```{r echo=FALSE}  
x <- c(1545, 1440, 1440, 1520, 1580, 1540, 1555, 1490, 1560, 1495, 1595, 1550, 1605, 1510, 1560, 1445, 1440, 1595, 1465, 1545, 1595, 1630, 1515, 1635, 1625, 1520, 1455, 1450, 1480, 1445)
x <- matrix(x, ncol=5, byrow=TRUE)
dimnames(x) <- list(1:6, paste0("S", 1:5))  
kable(x, row.names=TRUE)      
```

Let $y_{ij}$ denote the $j$th observation in batch $i$. To determine the relative importance of between-batch variation versus sampling variation, the following multilevel model is applied ($N$ denotes the number of batches and $n$ denotes the number of observations per batch).

- $y_{ij} \sim N(\mu + b_i, \sigma_y)$, $i=1,\ldots,N$, $j=1,\ldots,n$.
- $b_i \sim N(0, \sigma_b)$, $i=1,\ldots,N$.
- $(\sigma^2_y, \sigma^2_b)$ is assigned a uniform prior.

In this situation, the focus is on the marginal posterior distribution of the variance components. It is possible to analytically integrate out the random effects $b_i$s, resulting in the marginal posterior density of $(\mu, \sigma^2_y, \sigma^2_b)$ given, up to a proportionally constant, by

$$
\prod_{i=1}^N \left[ \phi\left( \bar{y}_i  \vert  \mu, \sqrt{\sigma^2_y/n + \sigma^2_b} \right) f_G\left( S_i  \vert  (n-1)/2, 1/(2 \sigma^2_y) \right) \right] \ , 
$$

where $\hat{y}_i$ and $S_i$ are respectively the mean yield and the "within sum of squares" of the $i$th batch, $\phi\left(y  \vert  \mu,\sigma\right)$ is the normal density of mean $\mu$ and standard deviation $\sigma$, and $f_G(y  \vert  a,b)$ is the gamma density proportional to $y^{a-1} \exp( - b y)$.

- Write an R function for the log of the posterior density with parametrization $\theta = (\mu, \log\sigma_y, \log\sigma_b)$.
- Using a normal approximation, to this aim,
   - find the posterior mode of $\theta$ using a numerical method and try the following alternative starting values
     - $\theta=(1500, 3, 3)$
     - $\theta=(1500, 1, 1)$
     - $\theta=(1500, 10, 10)$
  
     and assess the sensitivity of the numerical method to the starting value.
- Use the normal approximation to find $90\%$ interval estimates for the log of the standard deviation $\log\sigma_y$, $\log\sigma_b$.
- Using the results from the previous point find $90\%$ interval estimates for the variance components $\sigma_y^2$, $\sigma_b^2$.


## Write an R function to compute the posterior log density with parametrization $\theta = (\mu, \log\sigma_y, \log\sigma_b)$.

```{r echo=TRUE}
# Define some parameters
N <- length(x[,1])          # number of batches
n <- length(x[1,])          # number of data in each batch
mu <- rowMeans(x)           # mean value for each batch
S <- rowSums((x-mu)^2)      # within sum of squares for each batch
```

Let $N=$ `r N` be the number of batches and $n=$ `r n` the number of observations in each batch.
The posterior density is given by:
\begin{equation}
    g(\mu, \sigma_y^2, \sigma_b^2 \vert data) = \prod_{i=1}^{N} \left[ \phi \left(\bar{y_i} \vert \mu, \sqrt{\frac{\sigma_y^2}{n} + \sigma_b^2} \right) 
    f_G \left(S_i \vert \frac{n-1}{2}, \frac{1}{2\sigma_y^2} \right) \right]
\end{equation}

In order to change parametrization into $\theta = (\theta_1, \theta_2, \theta_3) = (\mu, \log\sigma_y, \log\sigma_b)$ the following Jacobian is needed

\begin{align}
    \mathbf{J} &=
    \begin{bmatrix}
    1 & 0 & 0 \\
    0 & \frac{1}{2\sigma_y^2} & 0 \\
    0 & 0 & \frac{1}{2\sigma_b^2} \\
    \end{bmatrix} 
    & \Longrightarrow & |J| = \frac{1}{4\sigma_y^2\sigma_b^2} & \Longrightarrow & |J|^{-1} = 4\sigma_y^2\sigma_b^2  
\end{align}

Changing variable:
\begin{align}
    g(\theta \vert data) &\propto \prod_{i=1}^{N} \left[ \phi \left(\bar{y_i} \vert \theta_1, \sqrt{\frac{e^{2\theta_2}}{n} + e^{2\theta_3}} \right) 
    f_G \left(S_i \vert \frac{n-1}{2}, \frac{1}{2}e^{-2\theta_2} \right) (4 e^{2(\theta_2+\theta_3)}) \right] \\
\end{align}

In order to simplify the notation, let's introduce
\begin{align}
    \sigma = \sigma(\theta_2, \theta_3) &= \sqrt{\frac{e^{2\theta_2}}{n} + e^{2\theta_3}} & a &= \frac{n-1}{2} & b = b(\theta_2) &= \frac{1}{2}e^{-2\theta_2}  
\end{align}

With this notation:
\begin{align}
    g(\theta \vert data) &\propto e^{2N(\theta_2+\theta_3)} \prod_{i=1}^{N} \left[ \phi \left(\bar{y_i} \vert \theta_1, \sigma \right) 
    f_G \left(S_i \vert a, b \right) \right]
\end{align}

And thus the posterior log (up to an additive constant):
\begin{equation}
    \log(g(\theta \vert data)) = 2N(\theta_2+\theta_3) + \sum_{i=1}^{N} \log(\phi(\bar{y_i} \vert \theta_1, \sigma)) + \sum_{i=1}^{N} \log(f_G(S_i \vert a, b))
\end{equation}


```{r echo=TRUE}
# Log-Posterior density
logPost <- function(theta){
    # extract theta
    t1 <- theta[1]
    t2 <- theta[2]
    t3 <- theta[3]
    # parameters
    a <- (n-1) / 2
    var <- exp(2*t2)/n + exp(2*t3)
    b <- exp(-2*t2) / 2
    # calculate the result
    res <- 2*N*(t2+t3)
    res <- sum(dnorm(mu, mean=t1, sd=sqrt(var), log=TRUE))
    res <- res + sum(dgamma(S, shape = a, rate = b, log = TRUE))   
    return (res)
}
```

## Normal approaximation

```{r echo=FALSE}
scaling <- 1
```

First attempt
```{r echo=TRUE, warning=FALSE}
start1        <- c(1500, 3, 3)
optimization1 <- optim(par=start1, fn=logPost, control=list(fnscale=-scaling), method="CG", hessian = TRUE)
mode1         <- optimization1$par
increment1    <- abs(mode1-start1)
```

Second attempt
```{r echo=TRUE, warning=FALSE}
start2        <- c(1500, 1, 1)
optimization2 <- optim(par=start2, fn=logPost, control=list(fnscale=-scaling), method="CG", hessian = TRUE)
mode2         <- optimization2$par
increment2    <- abs(mode2-start2)
```

Third attempt
```{r echo=TRUE, warning=FALSE}
start3        <- c(1500, 10, 10)
optimization3 <- optim(par=start3, fn=logPost, control=list(fnscale=-scaling), method="CG", hessian = TRUE)
mode3         <- optimization3$par
increment3    <- abs(mode3-start3)
```

The following results are obtained

| Starting value | Mode               |
|----------------|--------------------|
| (1500, 3, 3)   | `r round(mode1,4)` |
| (1500, 1, 1)   | `r round(mode2,4)` |
| (1500, 10, 10) | `r round(mode3,4)` |

So, the $3$ initial conditions yield approximately the same result.

## Assess the sensitivity of the numerical method to the starting value

The obtained results are stable under small variations of the corresponding initial conditions.
This was checked slightly perturbing each initial condition and calculating the sensitivity coefficients.  

## Find a 90% interval estimates for the log of the standard deviations and for the variances

Calculate a $90\%$ confidence interval for $\theta_2=\log\sigma_y$ and $\theta_3=\log\sigma_b$

```{r echo=TRUE}
mode <- mode1
V <- -solve(optimization1$hessian)
lowery <- mode[2]-qnorm(0.95)*sqrt(V[2,2])
uppery <- mode[2]+qnorm(0.95)*sqrt(V[2,2])
lowerb <- mode[3]-qnorm(0.95)*sqrt(V[3,3])
upperb <- mode[3]+qnorm(0.95)*sqrt(V[3,3])
```

| Parameter      | $90\%$ confidence interval         |
|----------------|------------------------------------|
| $\log\sigma_y$ | $[`r round(c(lowery, uppery),4)`]$ |
| $\log\sigma_b$ | $[`r round(c(lowerb, upperb),4)`]$ |

From which the $90\%$ confidence interval for the $\sigma_y^2$ and $\sigma_b^2$

```{r echo=TRUE}
lowervary <- exp(2*lowery)
uppervary <- exp(2*uppery)
lowervarb <- exp(2*lowerb)
uppervarb <- exp(2*upperb)
```

| Parameter    | $90\%$ confidence interval |
|--------------|----------------------------|
| $\sigma_y^2$ | $[`r round(c(lowervary, uppervary),4)`]$  |
| $\sigma_b^2$ | $[`r round(c(lowervarb, uppervarb),4)`]$  |

<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# References
