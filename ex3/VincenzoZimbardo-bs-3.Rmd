---
title: Homework 3 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
bibliography: references.bib
---

# Exercise 1: Mixture of exponential data

```{r echo=FALSE}
set.seed(12345)
```

Suppose a company obtains boxes of electronic parts from a particular supplier. 
It is known that $80\%$ of the lots are acceptable and the lifetimes of the "acceptable" parts follow an exponential distribution with mean $\lambda_A$.
Unfortunately, $20\%$ of the lots are unacceptable and the lifetimes of the "bad" parts are exponential with mean $\lambda_B$, 
where $\lambda_A > \lambda_B$. Suppose $y_1, \ldots, y_n$, are the lifetimes of $n$ inspected parts that can come from either acceptable
and unacceptable lots. 

The following lifetimes are observed from a sample of $30$ parts:
```{r echo=TRUE}
# Lifetimes data 
data <- c(0.98, 0.29, 36.70, 10.39, 39.93)
data <- c(data, c(14.57, 1.67, 18.81, 4.87, 20.24))
data <- c(data, c(0.08, 7.08, 14.89, 18.64, 8.69))
data <- c(data, c(0.18, 32.21, 0.16, 1.46, 0.58))
data <- c(data, c(86.49, 18.51, 0.72, 2.69, 2.58))
data <- c(data, c(41.79, 50.38, 0.77, 24.60, 0.91))
```


The $y_i$s are a random sample from the mixture distribution
$$  
h(y \vert  \lambda_A, \lambda_B) = p \frac{\exp(-y/\lambda_A)}{\lambda_A} + (1-p) \frac{\exp(-y/\lambda_B)}{\lambda_B} \ ,
$$
where $p=0.8$. Suppose $(\lambda_A, \lambda_B)$ are assigned the noninformative prior proportional to $1/(\lambda_A \ \lambda_B)$. 
The following function \texttt{log.exponential.mix} computes the log posterior density of the transformed parameters 
$\theta = (\theta_A, \theta_B) = (\log \lambda_A, \log \lambda_B)$:

```{r echo=TRUE}  
log.exponential.mix <- function(theta) {
  lambda.A <- exp(theta[1])
  lambda.B <- exp(theta[2])  
  res <- sum(log(0.8*dexp(data,1/lambda.A)+(1-0.8)*dexp(data,1/lambda.B))) 
  return (res)
}
```
    
### Construct a contour plot of $(\theta_A, \theta_B)$ over the rectangle $(1,4) \times (-2,8)$

```{r echo=TRUE}
# Create data
x <- seq(1, 4, 0.01)
y <- seq(-2, 8, 0.01)
z <- matrix(nrow = length(x), ncol = length(y))

for (i in 1:length(x)) {
  for (j in 1:length(y)) {
    z[i, j] <- log.exponential.mix(theta = c(x[i], y[j]))
  }
}

# Create the contour plot
contour(x, y, z, main = "Contour Plot", nlevels=15, xlab = expression(theta[A]), ylab = expression(theta[B]))
```

### Using the function \texttt{optim} search for the posterior mode with a starting guess of $(\theta_A, \theta_B) = (3,0)$.

```{r echo=TRUE}
start1        <- c(3, 0)
optimization1 <- optim(par=start1, fn=log.exponential.mix, control=list(fnscale=-1), hessian = TRUE)
mode1         <- optimization1$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode1[1],4), ",", round(mode1[2],4), ")"))
```

### Search for the posterior mode with a starting guess $(\theta_A, \theta_B) = (2,4)$.

```{r echo=TRUE}
start2        <- c(2, 4)
optimization2 <- optim(par=start2, fn=log.exponential.mix, control=list(fnscale=-1), hessian = TRUE)
mode2         <- optimization2$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode2[1],4), ",", round(mode2[2],4), ")"))
```

### Explain why you obtain different estimates of the posterior mode in the previous two points.  

The default method in the `optim` function in `R` is the one of @10.1093/comjnl/7.4.308, which is a deterministic numerical method. 
It assumes the existence of a unique maximum in the area of the search, as explained by the authors.
For this reason, the two different initial conditions give rise to two different results. As it is possible to note from the contour plot,
with the first initial condition the algorithm converges to the local minimum in the lower-right part of the plot, while the second initial condition is driven
away from it, toward another local minumum.

To overcome this problem, in the case of multimodal function to be optimized, stochastic methods are used instead.
Indeed, using the `SANN` method in the function `optim`, which implement a simulated annealing algorithm, the following results are obtained: 

```{r echo=TRUE}
optimization <- optim(par=start1, fn=log.exponential.mix, control=list(fnscale=-1), method="SANN", hessian=TRUE)
mode         <- optimization$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode[1],4), ",", round(mode[2],4), ")"))
```

```{r echo=TRUE}
optimization <- optim(par=start2, fn=log.exponential.mix, control=list(fnscale=-1), method="SANN", hessian=TRUE)
mode         <- optimization$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode[1],4), ",", round(mode[2],4), ")"))
```

From that it can be deduced which one is the global minimum.

### Use a normal approximation to construct a random walk Metropolis chain for sampling the posterior of $\theta= (\log(\lambda_A), \log(\lambda_B))$. Run the chain for $10000$ iterations, and costruct density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$.

```{r echo=TRUE}
# Laplace approximation
mode <- mode1
V <- -solve(optimization1$hessian)
```

Implementation of the random walk Metropolis-Hastings algorithm
```{r echo=TRUE}
library(MASS)
# target distribution
target <- function(x){
  return (exp(log.exponential.mix(x)))
}
# Random Walk Metropolis-Hastings
RWMH <- function(x_start, target, n){
  samples <- matrix(nrow=n+1, ncol=2)
  samples[1,] <- x_start
  x_curr <- x_start
  # iteration
  for (i in 1:n){
    # sample from proposal
    z_prop <- mvrnorm(n=1, mu=c(0,0), Sigma=V)
    # calculate acceptance probability
    x_prop <- x_curr + z_prop
    acceptance_prob <- min(1, target(x_prop) / target(x_curr))
    # accept or reject the proposal step
    if (runif(1) < acceptance_prob) {
      x_curr <- x_prop
    }
    samples[i+1,] <- x_curr
  }
  # return samples
  return (samples)
}
```

Sampling
```{r echo=TRUE}
# Set the parameters for the Random Walk Metropolis-Hastings algorithm
x_start <- c(3,0)  # Initial value of the Markov chain
n <- 10000  # Number of iterations
# Run the Random Walk Metropolis-Hastings algorithm
samples <- RWMH(x_start, target, n)
samples <- samples[-1,]
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 2))
plot(1:n,samples[,1], type="l", xlab = expression(i), ylab = expression(theta[A]^(i)))
plot(1:n,samples[,2], type="l", xlab = expression(i), ylab = expression(theta[B]^(i)))
# Restore default plot options
par(mfrow = c(1, 1))
```

From the trace plots is is possible to deduce that no burn-in phase is necessary. The reason for that is that a good starting point 
(the mode obtained from the Laplace's approximation) has been used. 
On the other hand, if another initial condition is chosen (for example in the proximity $(2,4)$ of the other local maximum) a burn-in phase could not be optional. 

Correlation analysis
```{r echo=TRUE}
acf(samples)
```

In order to reduce the autocorrelation in the sample we could use a thinning procedure (increasing the computation time) or try to randomly permute the obtained sequence:
```{r echo=TRUE}
# permute the samples to lower the autocorrelation
samples[,1] <- sample(samples[,1])
samples[,2] <- sample(samples[,2])
acf(samples)
```

In this way the autocorrelation is drastically reduced and the sample can be considered as an IID sample.

The final sample is summarized in the following $2D$ histogram:
```{r echo=FALSE, message=FALSE}
library(gplots)
hist_obj <- hist2d(samples, col=c("aliceblue", heat.colors(12)), main="RWMH sample histogram", xlab = expression(theta[A]), ylab = expression(theta[B]))
```

Finally, density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$ can be constructed:

```{r echo=TRUE}
theta_A <- mean(samples[,1])
theta_B <- mean(samples[,2])
```

```{r echo=FALSE}
print(paste0("theta_A", ":", round(theta_A,4)))
print(paste0("theta_B", ":", round(theta_B,4)))
```


### Construct a Metropolis within Gibbs samples, i.e., use a Metropolis algorithm to sample from $\log(\lambda_B) \lvert \log(\lambda_A)$ and then do the viceversa. Also run the chain for $10000$ iterations and costruct density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$.   

The Gibbs sampler is a MCMC algorithm in which each variable is sampled from its full conditional distribution while keeping the other variables fixed
at their current values. This means that in each iteration, every variable in the model is updated one at a time, sequentially, using its conditional
distribution given the current values of the other variables. The variables are updated in a cyclic manner until convergence is reached.

Gibbs sampling is the optimal choice when the full conditional distributions are easily calculated and sampling from them is feasible.
In this case it is not possible to easily calculate the full conditional distributions, so an implementation of the Metropolis within Gibbs is a better option. 

The Metropolis within Gibbs algorithm is a variation of the Gibbs sampler: it also samples each variable from its conditional distribution given the other
variables. However, instead of directly sampling from the conditional distribution, it employs the Metropolis-Hastings algorithm for acceptance/rejection of proposed values.
The Metropolis within Gibbs algorithm introduces acceptance/rejection steps to handle situations where the conditional distribution of a variable is not
readily available or it is difficult to sample directly from it.

The Metropolis within Gibbs algorithm consists in the following steps (CHECK!)

1. Select a variable from the set of variables
2. Propose a new value for the selected variable based on its conditional distribution
3. Calculate the acceptance probability for the proposed value using the Metropolis-Hastings ratio
4. Accept or reject the proposed value based on the acceptance probability
5. Repeat until convergence is reached

In order to sample from the full conditionals, we will need appropriate proposal distribution.
To this end, we will use the conditional distribution obtained from the Laplace's approximation, that we have already calculated:

\begin{equation}
  (\theta_A, \theta_B) \sim \mathcal{N}_2 (\mu, \Sigma)
\end{equation}
With 
\begin{align}
  \mu &= 
    \begin{pmatrix}
      \mu_A \\
      \mu_B \\
    \end{pmatrix} 
  & 
  \Sigma &= 
    \begin{bmatrix}
      \sigma_{1}^2 & \sigma_{1}\sigma_{2}\rho \\
      \sigma_{1}\sigma_{2}\rho & \sigma_{2}^2 \\
    \end{bmatrix}
\end{align}

The conditional distributions are again normal distributions, with parameters: 

\begin{equation}
  (\theta_A | \theta_B) \sim \mathcal{N}_1 \left(\mu_A + \frac{\sigma_1 \rho}{\sigma_2}(\theta_B-\mu_B), \sigma_1^2(1-\rho^2)\right)
\end{equation}

And similarly for $(\theta_B | \theta_A)$, just exchange $\theta_B \leftrightarrow \theta_A$.


```{r echo=TRUE}
# Function to sample from the conditional distribution of theta_A given theta_B
sample_A_given_B <- function(B, A_curr) {
  # proposal parameter
  mu <- mode[1] + (B-mode[2]) * V[1,2] / V[2,2]
  var <- V[1,1] - V[1,2]^2 / V[2,2] 
  # proposal density
  A_prop <- rnorm(1, mean=mu, sd=sqrt(var))
  acceptance_prob <- min(1, target(c(A_prop, B)) * dnorm(A_curr, mean=mu, sd=sqrt(var)) / ( target(c(A_curr, B)) * dnorm(A_prop, mean=mu, sd=sqrt(var)) ))
  if (runif(1) < acceptance_prob){
    return(A_prop)  # Accept the proposal
  }else{
    return(A_curr)  # Reject the proposal
  }
}
# Function to sample from the conditional distribution of theta_B given theta_A
sample_B_given_A <- function(A, B_curr) {
  # proposal parameter
  mu <- mode[2] + (A-mode[1]) * V[1,2] / V[1,1]
  var <- V[2,2] - V[1,2]^2 / V[1,1] 
  # proposal density
  B_prop <- rnorm(1, mean=mu, sd=sqrt(var))
  acceptance_prob <- min(1, target(c(A, B_prop)) * dnorm(B_curr, mean=mu, sd=sqrt(var)) / ( target(c(A, B_curr)) * dnorm(B_prop, mean=mu, sd=sqrt(var)) ))
  if (runif(1) < acceptance_prob){
    return(B_prop)  # Accept the proposal
  }else{
    return(B_curr)  # Reject the proposal
  }
}
# Function to perform Metropolis within Gibbs sampling
metropolis_within_gibbs <- function(n, A_start, B_start) {
  samples <- matrix(nrow = n, ncol = 2)
  A_curr <- A_start
  B_curr <- B_start
  for (i in 1:n) {
    A_curr <- sample_A_given_B(B_curr, A_curr)
    B_curr <- sample_B_given_A(A_curr, B_curr)
    samples[i, ] <- c(A_curr, B_curr)
  }
  return(samples)
}
```

Perform sampling
```{r echo=TRUE}
# Set the initial values and number of samples
A_start <- 3
B_start <- 0
n <- 10000
# Perform Metropolis within Gibbs sampling
samples <- metropolis_within_gibbs(n, A_start, B_start)
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 2))
plot(1:n,samples[,1], type="l", xlab = expression(i), ylab = expression(theta[A]^(i)))
plot(1:n,samples[,2], type="l", xlab = expression(i), ylab = expression(theta[B]^(i)))
# Restore default plot options
par(mfrow = c(1, 1))
```

Correlation analysis

#### Actually, it does not make much sense for the calculation of the arithmetic mean. Do stuff of the integrated autocorrelation time and effective sample size. Also in the previous part

```{r echo=TRUE}
acf(samples)
```

In this case neither burn-in phase nor autocorrelation reduction are needed, and we can regard the sample as an IID sample.

```{r echo=FALSE, message=FALSE}
library(gplots)
hist_obj <- hist2d(samples, col=c("aliceblue", heat.colors(12)), main="RWMH sample histogram", xlab = expression(theta[A]), ylab = expression(theta[B]))
```

Finally, density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$ can be constructed:

```{r echo=TRUE}
theta_A <- mean(samples[,1])
theta_B <- mean(samples[,2])
```

```{r echo=FALSE}
print(paste0("theta_A", ":", round(theta_A,4)))
print(paste0("theta_B", ":", round(theta_B,4)))
```

We conclude noticing that the two sampling algorithms (RWMH and MwG) give very similar results.

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```


<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# Exercise 2: Birthweight regression

Dobson (2001) describes a birthweight regression study. One is interested in predicting a baby's birthweight (in grams) based on the gestational age (in weeks) and the gender of the baby. 
The data are available as `birthweight` in the `LearnBayes` R package. In the standard linear regression model, we assume that

$$
BIRTHWEIGHT_i = \beta_0 + \beta_1 AGE_i + \beta_2 GENDER_i + \epsilon_i
$$

Data extraction
```{r echo=TRUE}
library(LearnBayes)
data <- LearnBayes::birthweight
str(data)
```

### Use the R function `lm` to fit this model by least-squares. From the output, assess if the effects `AGE` and `GENDER` are significant, and if they are significant, describe the effects of each covariate on `BIRTHWEIGHT`.

```{r echo=TRUE}
fit <- lm(weight ~ age + gender, data=data)
summary(fit)
```

From the p-values we deduce that `AGE` is the most significant covariate. Its value is positive, as expected, 
since the increasing age of the child should naturally lead to an increase in weight. 

### Suppose a noninformative [Zellner's g prior](https://en.wikipedia.org/wiki/G-prior) is placed on the regression parameter vector $\beta = (\beta_0,\beta_1,\beta_2)$ and assume an Inverse-Gamma priori for $\sigma^2$.

Let's rewrite the linear regression model as

\begin{align}
  y_i &= x_i^T \beta + \epsilon_i & \epsilon_i &\sim \mathcal{N}(0,\sigma^2) & \beta&=(\beta_0, \beta_1, \beta_2)^T
\end{align}

It follows that

\begin{equation}
  y_i | \beta, \sigma^2 \sim \mathcal{N}(x_i^T\beta, \sigma^2)
\end{equation}

Thus the likelihood for the model is (with $N$ equal to the number of observations):

\begin{equation}
  f(\mathbf{y} | \beta, \sigma^2) = (2\pi\sigma^2)^{-N/2} \exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i - x_i^T\beta)^2\right]}
\end{equation}

For the priors we consider the Zellner's g-prior for $\beta|\sigma^2$ and an inverse-gamma prior for $\sigma^2$

\begin{align}
  \beta|\sigma^2 &\sim \mathcal{N}_3(B, g\sigma^2 (X^T X)^{-1}) & \sigma^2 &\sim \mathcal{IG}(a,b)
\end{align}

Where $g>0$ and $X$ is the matrix with $i-th$ row equal to $x_i^T$.

From Bayes's theorem:

\begin{equation}
  f(\beta, \sigma^2 | \textbf{y}) \propto f(\textbf{y} | \beta, \sigma^2) f(\beta, \sigma^2) = f(\textbf{y} | \beta, \sigma^2) f(\beta | \sigma^2) f(\sigma^2)
\end{equation}

About the choice of the prior hyper-parameters: 

The mean of the g-prior $B$ is a vector that centers $\beta_1$ to the value given by the least-squares fit (as it is a significant value, given its p-value),
while centers in zero the parameters $\beta_0$ and $\beta_1$, for which the least-squares fit provides a large p-value.
The value of $a,b,g$ are chosen in such a way to have a quite large variance, thus without putting any strong prior assumption about the localization of the parameters. 

```{r echo=TRUE}
library(invgamma)
library(mvtnorm)
X <- cbind(rep(1,nrow(data)), data[,1], data[,2])
y <- data[,3]
XX <- t(X) %*% X
Y <- solve(XX)
N <- nrow(X)
# Prior parameters
a <- 10
b <- 100
g <- 1000
coeff <- as.numeric(fit$coefficients["age"])
B <- c(0,coeff,0)
# prior on the variance sigma^2
logprior_sigma2 <- function(sigma2){
  return (log(dinvgamma(sigma2, shape=a, rate=b)))
}
# Zellner's g-prior on beta | sigma^2
logprior_beta <- function(beta, sigma2){
  stopifnot(length(beta) == 3)
  S <- g*sigma2 * Y
  return ( dmvnorm(beta, B, S, log=TRUE) )
}
# Loglikelihood
loglik <- function(beta, sigma2){
  stopifnot(length(beta) == 3)
  mu <- X %*% beta
  f <- dnorm(y, mean = mu, sd = sqrt(sigma2), log=TRUE)
  return(sum(f))
}
# Log posterior kernel
logposterior <- function(x){
  beta <- x[-4]
  sigma2 <- x[4]
  stopifnot(length(beta) == 3)
  return (loglik(beta, sigma2) + logprior_beta(beta, sigma2) + logprior_sigma2(sigma2)) 
}
```

### Simulate a sample of $5000$ draws from the joint posterior distribution of $(\beta,\sigma)$  

Implementation of a random-walk Metropolis-Hastings algorithm.

Evaluation of the Laplace's approximation to use as a proposal
```{r echo=TRUE}
start   <- c(B,1) 
laplace <- optim(par=start, fn=logposterior, control=list(fnscale=-1), hessian=TRUE)
mode    <- laplace$par
V       <- -solve(laplace$hessian) 
```

```{r echo=TRUE}
# Random Walk Metropolis-Hastings
RWMH <- function(x_start, logtarget, n){
  samples <- matrix(nrow=n+1, ncol=4)
  samples[1,] <- x_start
  x_curr <- x_start
  # iteration
  for (i in 1:n){
    # sample from proposal
    z_prop <- mvrnorm(n=1, mu=c(0,0,0,0), Sigma=V)
    # calculate acceptance probability
    x_prop <- x_curr + z_prop
    # acceptance_prob <- min(1, target(x_prop) / target(x_curr))
    acceptance_prob <- min(0, logtarget(x_prop) - logtarget(x_curr))
    # accept or reject the proposal step
    if (log(runif(1)) < acceptance_prob) {
      x_curr <- x_prop
    }
    samples[i+1,] <- x_curr
  }
  # return samples
  samples <- samples[-1,]
  return (samples)
}
```

Sampling
```{r echo=TRUE}
burn_in <- 500
n <- 5000
x_start <- c(B,1)  # Initial value of the Markov chain
# Run the Random Walk Metropolis-Hastings algorithm
samples <- RWMH(x_start, logposterior, n+burn_in)
samples <- samples[-(1:burn_in),]
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 4))
plot(1:n, samples[,1], type="l", xlab = expression(i), ylab = expression(beta[0]))
plot(1:n, samples[,2], type="l", xlab = expression(i), ylab = expression(beta[1]))
plot(1:n, samples[,3], type="l", xlab = expression(i), ylab = expression(beta[2]))
plot(1:n, samples[,4], type="l", xlab = expression(i), ylab = expression(sigma^2))
# Restore default plot options
par(mfrow = c(1, 1))
```

Correlation analysis
```{r echo=TRUE}
acf(samples)
```

```{r echo=FALSE}
par(mfrow = c(1,4))
hist(samples[,1], main=NULL, xlab=expression(beta[0]))
hist(samples[,2], main=NULL, xlab=expression(beta[1]))
hist(samples[,3], main=NULL, xlab=expression(beta[2]))
hist(sqrt(samples[,4]), main=NULL, xlab = expression(sigma))
par(mfrow = c(1, 1))
```

Mean values: beta0=`r mean(samples[,1])`, beta1=`r mean(samples[,2])`, beta2=`r mean(samples[,3])`, sigma2=`r mean(samples[,4])`. 

The lenght of the burn-in period was chosen looking at the trace plots.
Given the high correlation structure of the sample, we do not expect the estimation to be reliable, especially for $\sigma^2$.
To improve the accuracy, some correlation reduction technique should be employed, such as thinning, leading to a substantial increase of the computation time.

As an alternative, we can note that

\begin{equation}
  f(\beta, \sigma^2) = f(\beta | \sigma^2) f(\sigma^2)
\end{equation}

With $\sigma^2 \sim \mathcal{IG}(a,b)$ and $\beta | \sigma^2$ is the posterior associated to the Zellner's g-prior, 
that is $\beta | \sigma^2 \sim \mathcal{N}_3(q\hat{B}+(1-q)B, q\sigma^2(X^TX)^{-1})$, where $q=g/(1+g)$ and $\hat{B}=(X^TX)^{-1}X^Ty$.
Thus we can directly sample from these two distributions.

```{r echo=TRUE}
q <- g / (1+g)
Bhat <- Y %*% t(X) %*% y
mean <- q*Bhat + (1-q)*B
joint_sample <- function(n){
  # sample sigma^2
  s2 <- rinvgamma(n, shape=a, rate=b)
  b <- matrix(0, nrow=n, ncol=3)
  # sample beta given the value of sigma^2
  for (i in 1:n){
    S <- q * s2[i] * Y
     b[i,] <- rmvnorm(1, mean, S)
  }
  # return
  return (cbind(b,s2))
}
```

Sample again
```{r echo=TRUE}
samples <- matrix(0, nrow=n, ncol=4)
samples <- joint_sample(n)
```

```{r echo=FALSE}
par(mfrow = c(1,4))
hist(samples[,1], main=NULL, xlab=expression(beta[0]))
hist(samples[,2], main=NULL, xlab=expression(beta[1]))
hist(samples[,3], main=NULL, xlab=expression(beta[2]))
hist(sqrt(samples[,4]), main=NULL, xlab = expression(sigma))
par(mfrow = c(1, 1))
```

Mean values: beta0=`r mean(samples[,1])`, beta1=`r mean(samples[,2])`, beta2=`r mean(samples[,3])`, sigma2=`r mean(samples[,4])`.

### Use the function \texttt{blinreg} in package `LearnBayes` to redo the simulation and compare the results.

```{r echo=TRUE}
prior <- list(b0=B, c0=g)
LBsimul <- blinreg(y, X, n, prior)
beta_simul <- LBsimul$beta
sigma_simul <- sqrt(LBsimul$sigma)
```

```{r echo=FALSE}
par(mfrow = c(1,4))
hist(beta_simul[,1], main=NULL, xlab=expression(beta[0]))
hist(beta_simul[,2], main=NULL, xlab=expression(beta[1]))
hist(beta_simul[,3], main=NULL, xlab=expression(beta[2]))
hist(sigma_simul, main=NULL, xlab = expression(sigma))
par(mfrow = c(1, 1))
```

Mean values: beta0=`r mean(beta_simul[,1])`, beta1=`r mean(beta_simul[,2])`, beta2=`r mean(beta_simul[,3])`, sigma2=`r mean(sigma_simul)`.

As expected, looking qualitatively at the histograms, it is possible to see how more consistent results are obtained for the second sample,
at least concerning the mean value. At the same time, a smaller variance is obtained.

### From the simulated samples, compute the posterior means and standard deviations of $\beta_1$ and $\beta_2$.

```{r echo=TRUE}
beta1 <- mean(samples[,2])
dbeta1 <- sd(samples[,2])
beta2 <- mean(samples[,3])
dbeta2 <- sd(samples[,3])

beta1_simul <- mean(beta_simul[,2])
dbeta1_simul <- sd(beta_simul[,2])
beta2_simul <- mean(beta_simul[,3])
dbeta2_simul <- sd(beta_simul[,3])
```

Second sample

| Parameter  | Mean value         | Standard deviation     |
|------------|--------------------|------------------------|
| $\beta_1$  | `r round(beta1,4)` | `r round(dbeta1,4)`    |
| $\beta_2$  | `r round(beta2,4)` | `r round(dbeta2,4)`    |


Using the `blinreg` function

| Parameter  | Mean value               | Standard deviation           |
|------------|--------------------------|------------------------------|
| $\beta_1$  | `r round(beta1_simul,4)` | `r round(dbeta1_simul,4)`    |
| $\beta_2$  | `r round(beta2_simul,4)` | `r round(dbeta2_simul,4)`    |



### Check the consistency of the posterior means and standard deviations with the least-squares estimates and associated standard errors from the `lm` run.

As anticipated, the mean values calculated with the MH algorithm, with an exception for $\sigma^2$, are quite similar to the results obtained with the `blinreg` function.
The deviations are due to the correlation structure of the sample.

The sample obtained from $f(\sigma^2)$ and $f(\beta | \sigma^2)$ gives very similar mean values, but with a smaller variance.

Finally, the sample obtained from the `blinreg` function is the only one for which not only the mean values, but also the variance is compatible with the esitmations given by the least-squares fit. 
This is not a surprise, since the `blinreg` function utilizes `lm` within it. 


### Suppose one is interested in estimating the expected `birthweight` for `male` and `female` babies of gestational weeks `36` and `40`. From the simulated draws of the posterior distribution construct 90\% interval estimates for 36-week males, 36-week females, 40-week males, and 40-week females.

```{r echo=TRUE}
beta0 <- mean(samples[,1])
CI_mean <- beta0 + beta1*c(36,36,40,40) + beta2*c(0,1,0,1)
CI_inf <- qnorm(0.05, mean=CI_mean, sd=c(dbeta1, dbeta2))
CI_sup <- qnorm(0.95, mean=CI_mean, sd=c(dbeta1, dbeta2))
CI <- cbind(CI_inf, CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```


### Compare the results you obained with those you can have from function `blinregexpected`.

```{r echo=TRUE}
X1 <- rbind(c(36,0),c(36,1),c(40,0),c(40,1))
X1 <- cbind(1, X1)
exp_response <- blinregexpected(X1, LBsimul)
# Confidence interval construction
exp_mean <- apply(exp_response, MARGIN=2, mean)
exp_sd <- apply(exp_response, MARGIN=2, sd) 
CI_inf <- qnorm(0.05, mean=exp_mean, sd=exp_sd)
CI_sup <- qnorm(0.95, mean=exp_mean, sd=exp_sd)
CI <- cbind(CI_inf, CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```

Consistent with the fact of having used a sample with larger variance (the `blinreg` sample). Write this better, or even better use the same sample in both points. 

### Suppose instead that one wishes to predict the birthweight for a 36- week male, a 36-week female, a 40-week male, and a 40-week female. Use the simulated data  from the posterior to construct 90\% prediction intervals for the birthweight for each type of baby.

```{r echo=TRUE}
pred_sample <- samples[,1:3]%*%t(X1) + rnorm(n)*samples[,4]
CI_mean<- apply(pred_sample, MARGIN=2, mean)
CI_sd <- apply(pred_sample, MARGIN=2, sd)
CI_inf <- qnorm(0.05, mean=CI_mean, sd=CI_sd)
CI_sup <- qnorm(0.95, mean=CI_mean, sd=CI_sd)
CI <- cbind(CI_inf,CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```

### Compare the results you obained with those you can have from function `blinregpred`

```{r echo=TRUE}
pred_response <- blinregpred(X1, LBsimul)
# Confidence interval construction
pred_mean <- apply(pred_response, MARGIN=2, mean)
pred_sd <- apply(pred_response, MARGIN=2, sd) 
CI_inf <- qnorm(0.05, mean=pred_mean, sd=pred_sd)
CI_sup <- qnorm(0.95, mean=pred_mean, sd=pred_sd)
CI <- cbind(CI_inf, CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```

```{r echo=FALSE}
# Clear workspace
# rm(list=ls())
```

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 3

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 4

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# References

<!--------------------------------------------------------------------------------------------------------------------------------------------------->