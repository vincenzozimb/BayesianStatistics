---
title: Homework 3 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
bibliography: references.bib
---

# Exercise 1: Mixture of exponential data

Suppose a company obtains boxes of electronic parts from a particular supplier. 
It is known that $80\%$ of the lots are acceptable and the lifetimes of the "acceptable" parts follow an exponential distribution with mean $\lambda_A$.
Unfortunately, $20\%$ of the lots are unacceptable and the lifetimes of the "bad" parts are exponential with mean $\lambda_B$, 
where $\lambda_A > \lambda_B$. Suppose $y_1, \ldots, y_n$, are the lifetimes of $n$ inspected parts that can come from either acceptable
and unacceptable lots. 

The following lifetimes are observed from a sample of $30$ parts:
```{r echo=TRUE}
# Lifetimes data 
data <- c(0.98, 0.29, 36.70, 10.39, 39.93)
data <- c(data, c(14.57, 1.67, 18.81, 4.87, 20.24))
data <- c(data, c(0.08, 7.08, 14.89, 18.64, 8.69))
data <- c(data, c(0.18, 32.21, 0.16, 1.46, 0.58))
data <- c(data, c(86.49, 18.51, 0.72, 2.69, 2.58))
data <- c(data, c(41.79, 50.38, 0.77, 24.60, 0.91))
```


The $y_i$s are a random sample from the mixture distribution
$$  
h(y \vert  \lambda_A, \lambda_B) = p \frac{\exp(-y/\lambda_A)}{\lambda_A} + (1-p) \frac{\exp(-y/\lambda_B)}{\lambda_B} \ ,
$$
where $p=0.8$. Suppose $(\lambda_A, \lambda_B)$ are assigned the noninformative prior proportional to $1/(\lambda_A \ \lambda_B)$. 
The following function \texttt{log.exponential.mix} computes the log posterior density of the transformed parameters 
$\theta = (\theta_A, \theta_B) = (\log \lambda_A, \log \lambda_B)$:

```{r echo=TRUE}  
log.exponential.mix <- function(theta) {
  lambda.A <- exp(theta[1])
  lambda.B <- exp(theta[2])  
  sum(log(0.8*dexp(data,1/lambda.A)+(1-0.8)*dexp(data,1/lambda.B))) 
}
```
    
### Construct a contour plot of $(\theta_A, \theta_B)$ over the rectangle $(1,4) \times (-2,8)$

```{r echo=TRUE}
# Create data
x <- seq(1, 4, 0.01)
y <- seq(-2, 8, 0.01)
z <- matrix(nrow = length(x), ncol = length(y))

for (i in 1:length(x)) {
  for (j in 1:length(y)) {
    z[i, j] <- log.exponential.mix(theta = c(x[i], y[j]))
  }
}

# Create the contour plot
contour(x, y, z, main = "Contour Plot", nlevels=15, xlab = expression(theta[A]), ylab = expression(theta[B]))
```

### Using the function \texttt{optim} search for the posterior mode with a starting guess of $(\theta_A, \theta_B) = (3,0)$.

```{r echo=TRUE}
start1        <- c(3, 0)
optimization1 <- optim(par=start1, fn=log.exponential.mix, control=list(fnscale=-1), hessian = TRUE)
mode1         <- optimization1$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode1[1],4), ",", round(mode1[2],4), ")"))
```

### Search for the posterior mode with a starting guess $(\theta_A, \theta_B) = (2,4)$.

```{r echo=TRUE}
start2        <- c(2, 4)
optimization2 <- optim(par=start2, fn=log.exponential.mix, control=list(fnscale=-1), hessian = TRUE)
mode2         <- optimization2$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode2[1],4), ",", round(mode2[2],4), ")"))
```

### Explain why you obtain different estimates of the posterior mode in the previous two points.  

The default method in the `optim` function in `R` is the one of @10.1093/comjnl/7.4.308, which is a deterministic numerical method. 
It assumes the existence of a unique maximum in the area of the search, as explained by the authors.
For this reason, the two different initial conditions give rise to two different results. As it is possible to note from the contour plot,
with the first initial condition the algorithm converges to the local minimum in the lower-right part of the plot, while the second initial condition is driven
away from it, toward another local minumum.

To overcome this problem, in the case of multimodal function to be optimized, stochastic methods are used instead.
Indeed, using the `SANN` method in the function `optim`, which implement a simulated annealing algorithm, the following results are obtained: 

```{r echo=TRUE}
optimization <- optim(par=start1, fn=log.exponential.mix, control=list(fnscale=-1), method="SANN", hessian=TRUE)
mode         <- optimization$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode[1],4), ",", round(mode[2],4), ")"))
```

```{r echo=TRUE}
optimization <- optim(par=start2, fn=log.exponential.mix, control=list(fnscale=-1), method="SANN", hessian=TRUE)
mode         <- optimization$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode[1],4), ",", round(mode[2],4), ")"))
```

From which we can deduce which is the global minumum.

### Use a normal approximation to construct a random walk Metropolis chain for sampling the posterior of $\theta= (\log(\lambda_A), \log(\lambda_B))$. Run the chain for $10000$ iterations, and costruct density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$.

```{r echo=TRUE}
# Laplace approximation
mode <- mode1
V <- -solve(optimization1$hessian)
```

Implementation of the random walk Metropolis-Hastings algorithm
```{r echo=TRUE}
library(mvtnorm)
# target distribution
target <- function(x){
  return (exp(log.exponential.mix(x)))
}
# proposal distribution
proposal <- function(x) {
  proposed_value <- x + rmvnorm(n = 1, mean = mode, sigma = V)
  return(proposed_value)
}
# Metropolis-Hastings update
RWMH <- function(x, target, proposal, n){
  samples <- matrix(nrow=n, ncol=2)
  current_value <- x
  accepted <- 0
  for (i in 1:n){
    # sample from proposal
    proposed_value <- proposal(current_value)
    # calculate acceptance ratio
    acceptance_ratio <- target(proposed_value) / target(current_value)
    # accept or reject the proposal step
    # print(acceptance_ratio)
    if (runif(1) < acceptance_ratio){
      current_value <- proposed_value
      accepted <- accepted + 1
    }
    samples[i,] <- current_value[]
  }
  # update acceptance ratio
  acceptance_rate <- accepted / n
  # return
  result <- list(samples=samples, acceptance_rate=acceptance_rate)
  return(result)
}
```

Sampling
```{r echo=TRUE}
# Set the parameters for the Random Walk Metropolis-Hastings algorithm
x <- c(3,0)  # Initial value of the Markov chain
n <- 10000  # Number of iterations
# Run the Random Walk Metropolis-Hastings algorithm
result <- RWMH(x, target, proposal, n)
# Get the sampled values and acceptance rate
samples <- result$samples
acceptance_rate <- result$acceptance_rate
# Print the acceptance rate
cat("Acceptance rate:", acceptance_rate, "\n")
```

### Construct a Metropolis within Gibbs samples, i.e., use a Metropolis algorithm to sample from $\log(\lambda_B) \lvert \log(\lambda_A)$ and then do the viceversa. Also run the chain for $10000$ iterations and costruct density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$.   



```{r echo=FALSE}
# Clear workspace
# rm(list=ls())
```

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 2

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 3

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 4

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# References

<!--------------------------------------------------------------------------------------------------------------------------------------------------->