---
title: Homework 3 - Bayesian Statistics 2023
author: Vincenzo Zimbardo 
output: html_document
bibliography: references.bib
---

# Exercise 1: Mixture of exponential data

```{r echo=FALSE}
set.seed(12345)
```

Suppose a company obtains boxes of electronic parts from a particular supplier. 
It is known that $80\%$ of the lots are acceptable and the lifetimes of the "acceptable" parts follow an exponential distribution with mean $\lambda_A$.
Unfortunately, $20\%$ of the lots are unacceptable and the lifetimes of the "bad" parts are exponential with mean $\lambda_B$, 
where $\lambda_A > \lambda_B$. Suppose $y_1, \ldots, y_n$, are the lifetimes of $n$ inspected parts that can come from either acceptable
and unacceptable lots. 

The following lifetimes are observed from a sample of $30$ parts:
```{r echo=TRUE}
# Lifetimes data 
data <- c(0.98, 0.29, 36.70, 10.39, 39.93)
data <- c(data, c(14.57, 1.67, 18.81, 4.87, 20.24))
data <- c(data, c(0.08, 7.08, 14.89, 18.64, 8.69))
data <- c(data, c(0.18, 32.21, 0.16, 1.46, 0.58))
data <- c(data, c(86.49, 18.51, 0.72, 2.69, 2.58))
data <- c(data, c(41.79, 50.38, 0.77, 24.60, 0.91))
```


The $y_i$s are a random sample from the mixture distribution
$$  
h(y \vert  \lambda_A, \lambda_B) = p \frac{\exp(-y/\lambda_A)}{\lambda_A} + (1-p) \frac{\exp(-y/\lambda_B)}{\lambda_B} \ ,
$$
where $p=0.8$. Suppose $(\lambda_A, \lambda_B)$ are assigned the noninformative prior proportional to $1/(\lambda_A \ \lambda_B)$. 
The following function \texttt{log.exponential.mix} computes the log posterior density of the transformed parameters 
$\theta = (\theta_A, \theta_B) = (\log \lambda_A, \log \lambda_B)$:

```{r echo=TRUE}  
log.exponential.mix <- function(theta) {
  lambda.A <- exp(theta[1])
  lambda.B <- exp(theta[2])  
  res <- sum(log(0.8*dexp(data,1/lambda.A)+(1-0.8)*dexp(data,1/lambda.B))) 
  return (res)
}
```
    
### Construct a contour plot of $(\theta_A, \theta_B)$ over the rectangle $(1,4) \times (-2,8)$

```{r echo=TRUE}
# Create data
x <- seq(1, 4, 0.01)
y <- seq(-2, 8, 0.01)
z <- matrix(nrow = length(x), ncol = length(y))

for (i in 1:length(x)) {
  for (j in 1:length(y)) {
    z[i, j] <- log.exponential.mix(theta = c(x[i], y[j]))
  }
}

# Create the contour plot
contour(x, y, z, main = "Contour Plot", nlevels=15, xlab = expression(theta[A]), ylab = expression(theta[B]))
```

### Using the function \texttt{optim} search for the posterior mode with a starting guess of $(\theta_A, \theta_B) = (3,0)$.

```{r echo=TRUE}
start1        <- c(3, 0)
optimization1 <- optim(par=start1, fn=log.exponential.mix, control=list(fnscale=-1), hessian = TRUE)
mode1         <- optimization1$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode1[1],4), ",", round(mode1[2],4), ")"))
```

### Search for the posterior mode with a starting guess $(\theta_A, \theta_B) = (2,4)$.

```{r echo=TRUE}
start2        <- c(2, 4)
optimization2 <- optim(par=start2, fn=log.exponential.mix, control=list(fnscale=-1), hessian = TRUE)
mode2         <- optimization2$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode2[1],4), ",", round(mode2[2],4), ")"))
```

### Explain why you obtain different estimates of the posterior mode in the previous two points.  

The default method in the `optim` function in `R` is the one of @10.1093/comjnl/7.4.308, which is a deterministic numerical method. 
It assumes the existence of a unique maximum in the area of the search, as explained by the authors.
For this reason, the two different initial conditions give rise to two different results. As it is possible to note from the contour plot,
with the first initial condition the algorithm converges to the local minimum in the lower-right part of the plot, while the second initial condition is driven
away from it, toward another local minumum.

To overcome this problem, in the case of multimodal function to be optimized, stochastic methods are used instead.
Indeed, using the `SANN` method in the function `optim`, which implement a simulated annealing algorithm, the following results are obtained: 

```{r echo=TRUE}
optimization <- optim(par=start1, fn=log.exponential.mix, control=list(fnscale=-1), method="SANN", hessian=TRUE)
mode         <- optimization$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode[1],4), ",", round(mode[2],4), ")"))
```

```{r echo=TRUE}
optimization <- optim(par=start2, fn=log.exponential.mix, control=list(fnscale=-1), method="SANN", hessian=TRUE)
mode         <- optimization$par
```

```{r echo=FALSE}
print(paste0("Mode (", round(mode[1],4), ",", round(mode[2],4), ")"))
```

From that it can be deduced which one is the global minimum.

### Use a normal approximation to construct a random walk Metropolis chain for sampling the posterior of $\theta= (\log(\lambda_A), \log(\lambda_B))$. Run the chain for $10000$ iterations, and costruct density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$.

```{r echo=TRUE}
# Laplace approximation
mode <- mode1
V <- -solve(optimization1$hessian)
```

Implementation of the random walk Metropolis-Hastings algorithm
```{r echo=TRUE}
library(MASS)
# target distribution
target <- function(x){
  return (exp(log.exponential.mix(x)))
}
# Random Walk Metropolis-Hastings
RWMH <- function(x_start, target, n){
  samples <- matrix(nrow=n+1, ncol=2)
  samples[1,] <- x_start
  x_curr <- x_start
  # iteration
  for (i in 1:n){
    # sample from proposal
    z_prop <- mvrnorm(n=1, mu=c(0,0), Sigma=V)
    # calculate acceptance probability
    x_prop <- x_curr + z_prop
    acceptance_prob <- min(1, target(x_prop) / target(x_curr))
    # accept or reject the proposal step
    if (runif(1) < acceptance_prob) {
      x_curr <- x_prop
    }
    samples[i+1,] <- x_curr
  }
  # return samples
  return (samples)
}
```

Sampling
```{r echo=TRUE}
# Set the parameters for the Random Walk Metropolis-Hastings algorithm
x_start <- c(3,0)  # Initial value of the Markov chain
n <- 10000  # Number of iterations
# Run the Random Walk Metropolis-Hastings algorithm
samples <- RWMH(x_start, target, n)
samples <- samples[-1,]
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 2))
plot(1:n,samples[,1], type="l", xlab = expression(i), ylab = expression(theta[A]^(i)))
plot(1:n,samples[,2], type="l", xlab = expression(i), ylab = expression(theta[B]^(i)))
# Restore default plot options
par(mfrow = c(1, 1))
```

From the trace plots is is possible to deduce that no burn-in phase is necessary. The reason for that is that a good starting point 
(the mode obtained from the Laplace's approximation) has been used. 
On the other hand, if another initial condition is chosen (for example in the proximity $(2,4)$ of the other local maximum) a burn-in phase could not be optional. 

Correlation analysis
```{r echo=TRUE}
acf(samples)
```

In order to reduce the autocorrelation in the sample we could use a thinning procedure (increasing the computation time) or try to randomly permute the obtained sequence:
```{r echo=TRUE}
# permute the samples to lower the autocorrelation
samples[,1] <- sample(samples[,1])
samples[,2] <- sample(samples[,2])
acf(samples)
```

In this way the autocorrelation is drastically reduced and the sample can be considered as an IID sample.

The final sample is summarized in the following $2D$ histogram:
```{r echo=FALSE, message=FALSE}
library(gplots)
hist_obj <- hist2d(samples, col=c("aliceblue", heat.colors(12)), main="RWMH sample histogram", xlab = expression(theta[A]), ylab = expression(theta[B]))
```

Finally, density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$ can be constructed:

```{r echo=TRUE}
theta_A <- mean(samples[,1])
theta_B <- mean(samples[,2])
```

```{r echo=FALSE}
print(paste0("theta_A", ":", round(theta_A,4)))
print(paste0("theta_B", ":", round(theta_B,4)))
```


### Construct a Metropolis within Gibbs samples, i.e., use a Metropolis algorithm to sample from $\log(\lambda_B) \lvert \log(\lambda_A)$ and then do the viceversa. Also run the chain for $10000$ iterations and costruct density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$.   

The Gibbs sampler is a MCMC algorithm in which each variable is sampled from its full conditional distribution while keeping the other variables fixed
at their current values. This means that in each iteration, every variable in the model is updated one at a time, sequentially, using its conditional
distribution given the current values of the other variables. The variables are updated in a cyclic manner until convergence is reached.

Gibbs sampling is the optimal choice when the full conditional distributions are easily calculated and sampling from them is feasible.
In this case it is not possible to easily calculate the full conditional distributions, so an implementation of the Metropolis within Gibbs is a better option. 

The Metropolis within Gibbs algorithm is a variation of the Gibbs sampler: it also samples each variable from its conditional distribution given the other
variables. However, instead of directly sampling from the conditional distribution, it employs the Metropolis-Hastings algorithm for acceptance/rejection of proposed values.
The Metropolis within Gibbs algorithm introduces acceptance/rejection steps to handle situations where the conditional distribution of a variable is not
readily available or it is difficult to sample directly from it.

The Metropolis within Gibbs algorithm consists in the following steps (CHECK!)

1. Select a variable from the set of variables
2. Propose a new value for the selected variable based on its conditional distribution
3. Calculate the acceptance probability for the proposed value using the Metropolis-Hastings ratio
4. Accept or reject the proposed value based on the acceptance probability
5. Repeat until convergence is reached

In order to sample from the full conditionals, we will need appropriate proposal distribution.
To this end, we will use the conditional distribution obtained from the Laplace's approximation, that we have already calculated:

\begin{equation}
  (\theta_A, \theta_B) \sim \mathcal{N}_2 (\mu, \Sigma)
\end{equation}
With 
\begin{align}
  \mu &= 
    \begin{pmatrix}
      \mu_A \\
      \mu_B \\
    \end{pmatrix} 
  & 
  \Sigma &= 
    \begin{bmatrix}
      \sigma_{1}^2 & \sigma_{1}\sigma_{2}\rho \\
      \sigma_{1}\sigma_{2}\rho & \sigma_{2}^2 \\
    \end{bmatrix}
\end{align}

The conditional distributions are again normal distributions, with parameters: 

\begin{equation}
  (\theta_A | \theta_B) \sim \mathcal{N}_1 \left(\mu_A + \frac{\sigma_1 \rho}{\sigma_2}(\theta_B-\mu_B), \sigma_1^2(1-\rho^2)\right)
\end{equation}

And similarly for $(\theta_B | \theta_A)$, just exchange $\theta_B \leftrightarrow \theta_A$.


```{r echo=TRUE}
# Function to sample from the conditional distribution of theta_A given theta_B
sample_A_given_B <- function(B, A_curr) {
  # proposal parameter
  mu <- mode[1] + (B-mode[2]) * V[1,2] / V[2,2]
  var <- V[1,1] - V[1,2]^2 / V[2,2] 
  # proposal density
  A_prop <- rnorm(1, mean=mu, sd=sqrt(var))
  acceptance_prob <- min(1, target(c(A_prop, B)) * dnorm(A_curr, mean=mu, sd=sqrt(var)) / ( target(c(A_curr, B)) * dnorm(A_prop, mean=mu, sd=sqrt(var)) ))
  if (runif(1) < acceptance_prob){
    return(A_prop)  # Accept the proposal
  }else{
    return(A_curr)  # Reject the proposal
  }
}
# Function to sample from the conditional distribution of theta_B given theta_A
sample_B_given_A <- function(A, B_curr) {
  # proposal parameter
  mu <- mode[2] + (A-mode[1]) * V[1,2] / V[1,1]
  var <- V[2,2] - V[1,2]^2 / V[1,1] 
  # proposal density
  B_prop <- rnorm(1, mean=mu, sd=sqrt(var))
  acceptance_prob <- min(1, target(c(A, B_prop)) * dnorm(B_curr, mean=mu, sd=sqrt(var)) / ( target(c(A, B_curr)) * dnorm(B_prop, mean=mu, sd=sqrt(var)) ))
  if (runif(1) < acceptance_prob){
    return(B_prop)  # Accept the proposal
  }else{
    return(B_curr)  # Reject the proposal
  }
}
# Function to perform Metropolis within Gibbs sampling
metropolis_within_gibbs <- function(n, A_start, B_start) {
  samples <- matrix(nrow = n, ncol = 2)
  A_curr <- A_start
  B_curr <- B_start
  for (i in 1:n) {
    A_curr <- sample_A_given_B(B_curr, A_curr)
    B_curr <- sample_B_given_A(A_curr, B_curr)
    samples[i, ] <- c(A_curr, B_curr)
  }
  return(samples)
}
```

Perform sampling
```{r echo=TRUE}
# Set the initial values and number of samples
A_start <- 3
B_start <- 0
n <- 10000
# Perform Metropolis within Gibbs sampling
samples <- metropolis_within_gibbs(n, A_start, B_start)
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 2))
plot(1:n,samples[,1], type="l", xlab = expression(i), ylab = expression(theta[A]^(i)))
plot(1:n,samples[,2], type="l", xlab = expression(i), ylab = expression(theta[B]^(i)))
# Restore default plot options
par(mfrow = c(1, 1))
```

Correlation analysis

#### Actually, it does not make much sense for the calculation of the arithmetic mean. Do stuff of the integrated autocorrelation time and effective sample size. Also in the previous part

```{r echo=TRUE}
acf(samples)
```

In this case neither burn-in phase nor autocorrelation reduction are needed, and we can regard the sample as an IID sample.

```{r echo=FALSE, message=FALSE}
library(gplots)
hist_obj <- hist2d(samples, col=c("aliceblue", heat.colors(12)), main="RWMH sample histogram", xlab = expression(theta[A]), ylab = expression(theta[B]))
```

Finally, density estimates for $\log(\lambda_A)$ and $\log(\lambda_B)$ can be constructed:

```{r echo=TRUE}
theta_A <- mean(samples[,1])
theta_B <- mean(samples[,2])
```

```{r echo=FALSE}
print(paste0("theta_A", ":", round(theta_A,4)))
print(paste0("theta_B", ":", round(theta_B,4)))
```

We conclude noticing that the two sampling algorithms (RWMH and MwG) give very similar results.

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```


<!-- ----------------------------------------------------------------------------------------------------------------------------------------------- -->

# Exercise 2: Birthweight regression

Dobson (2001) describes a birthweight regression study. One is interested in predicting a baby's birthweight (in grams) based on the gestational age (in weeks) and the gender of the baby. 
The data are available as `birthweight` in the `LearnBayes` R package. In the standard linear regression model, we assume that

$$
BIRTHWEIGHT_i = \beta_0 + \beta_1 AGE_i + \beta_2 GENDER_i + \epsilon_i
$$

Data extraction
```{r echo=TRUE}
library(LearnBayes)
data <- LearnBayes::birthweight
str(data)
```

### Use the R function `lm` to fit this model by least-squares. From the output, assess if the effects `AGE` and `GENDER` are significant, and if they are significant, describe the effects of each covariate on `BIRTHWEIGHT`.

```{r echo=TRUE}
fit <- lm(weight ~ age + gender, data=data)
summary(fit)
```

From the p-values we deduce that `AGE` is the most significant covariate. Its value is positive, as expected, 
since the increasing age of the child should naturally lead to an increase in weight. 

### Suppose a noninformative [Zellner's g prior](https://en.wikipedia.org/wiki/G-prior) is placed on the regression parameter vector $\beta = (\beta_0,\beta_1,\beta_2)$ and assume an Inverse-Gamma priori for $\sigma^2$.

Let's rewrite the linear regression model as

\begin{align}
  y_i &= x_i^T \beta + \epsilon_i & \epsilon_i &\sim \mathcal{N}(0,\sigma^2) & \beta&=(\beta_0, \beta_1, \beta_2)^T
\end{align}

It follows that

\begin{equation}
  y_i | \beta, \sigma^2 \sim \mathcal{N}(x_i^T\beta, \sigma^2)
\end{equation}

Thus the likelihood for the model is (with $N$ equal to the number of observations):

\begin{equation}
  f(\mathbf{y} | \beta, \sigma^2) = (2\pi\sigma^2)^{-N/2} \exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i - x_i^T\beta)^2\right]}
\end{equation}

For the priors we consider the Zellner's g-prior for $\beta|\sigma^2$ and an inverse-gamma prior for $\sigma^2$

\begin{align}
  \beta|\sigma^2 &\sim \mathcal{N}_3(B, g\sigma^2 (X^T X)^{-1}) & \sigma^2 &\sim \mathcal{IG}(a,b)
\end{align}

Where $g>0$ and $X$ is the matrix with $i-th$ row equal to $x_i^T$.

From Bayes's theorem:

\begin{equation}
  f(\beta, \sigma^2 | \textbf{y}) \propto f(\textbf{y} | \beta, \sigma^2) f(\beta, \sigma^2) = f(\textbf{y} | \beta, \sigma^2) f(\beta | \sigma^2) f(\sigma^2)
\end{equation}

About the choice of the prior hyper-parameters: 

The mean of the g-prior $B$ is a vector that centers $\beta_1$ to the value given by the least-squares fit (as it is a significant value, given its p-value),
while centers in zero the parameters $\beta_0$ and $\beta_1$, for which the least-squares fit provides a large p-value.
The value of $a,b,g$ are chosen in such a way to have a quite large variance, thus without putting any strong prior assumption about the localization of the parameters. 

```{r echo=TRUE}
library(invgamma)
library(mvtnorm)
X <- cbind(rep(1,nrow(data)), data[,1], data[,2])
y <- data[,3]
XX <- t(X) %*% X
Y <- solve(XX)
N <- nrow(X)
# Prior parameters
a <- 10
b <- 100
g <- 1000
coeff <- as.numeric(fit$coefficients["age"])
B <- c(0,coeff,0)
# prior on the variance sigma^2
logprior_sigma2 <- function(sigma2){
  return (log(dinvgamma(sigma2, shape=a, rate=b)))
}
# Zellner's g-prior on beta | sigma^2
logprior_beta <- function(beta, sigma2){
  stopifnot(length(beta) == 3)
  S <- g*sigma2 * Y
  return ( dmvnorm(beta, B, S, log=TRUE) )
}
# Loglikelihood
loglik <- function(beta, sigma2){
  stopifnot(length(beta) == 3)
  mu <- X %*% beta
  f <- dnorm(y, mean = mu, sd = sqrt(sigma2), log=TRUE)
  return(sum(f))
}
# Log posterior kernel
logposterior <- function(x){
  beta <- x[-4]
  sigma2 <- x[4]
  stopifnot(length(beta) == 3)
  return (loglik(beta, sigma2) + logprior_beta(beta, sigma2) + logprior_sigma2(sigma2)) 
}
```

### Simulate a sample of $5000$ draws from the joint posterior distribution of $(\beta,\sigma)$  

Implementation of a random-walk Metropolis-Hastings algorithm.

Evaluation of the Laplace's approximation to use as a proposal
```{r echo=TRUE}
start   <- c(B,1) 
laplace <- optim(par=start, fn=logposterior, control=list(fnscale=-1), hessian=TRUE)
mode    <- laplace$par
V       <- -solve(laplace$hessian) 
```

```{r echo=TRUE}
# Random Walk Metropolis-Hastings
RWMH <- function(x_start, logtarget, n){
  samples <- matrix(nrow=n+1, ncol=4)
  samples[1,] <- x_start
  x_curr <- x_start
  # iteration
  for (i in 1:n){
    # sample from proposal
    z_prop <- mvrnorm(n=1, mu=c(0,0,0,0), Sigma=V)
    # calculate acceptance probability
    x_prop <- x_curr + z_prop
    # acceptance_prob <- min(1, target(x_prop) / target(x_curr))
    acceptance_prob <- min(0, logtarget(x_prop) - logtarget(x_curr))
    # accept or reject the proposal step
    if (log(runif(1)) < acceptance_prob) {
      x_curr <- x_prop
    }
    samples[i+1,] <- x_curr
  }
  # return samples
  samples <- samples[-1,]
  return (samples)
}
```

Sampling
```{r echo=TRUE}
burn_in <- 500
n <- 5000
x_start <- c(B,1)  # Initial value of the Markov chain
# Run the Random Walk Metropolis-Hastings algorithm
samples <- RWMH(x_start, logposterior, n+burn_in)
samples <- samples[-(1:burn_in),]
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 4))
plot(1:n, samples[,1], type="l", xlab = expression(i), ylab = expression(beta[0]))
plot(1:n, samples[,2], type="l", xlab = expression(i), ylab = expression(beta[1]))
plot(1:n, samples[,3], type="l", xlab = expression(i), ylab = expression(beta[2]))
plot(1:n, samples[,4], type="l", xlab = expression(i), ylab = expression(sigma^2))
# Restore default plot options
par(mfrow = c(1, 1))
```

Correlation analysis
```{r echo=TRUE}
acf(samples)
```

```{r echo=FALSE}
par(mfrow = c(1,4))
hist(samples[,1], main=NULL, xlab=expression(beta[0]))
hist(samples[,2], main=NULL, xlab=expression(beta[1]))
hist(samples[,3], main=NULL, xlab=expression(beta[2]))
hist(sqrt(samples[,4]), main=NULL, xlab = expression(sigma))
par(mfrow = c(1, 1))
```

Mean values: beta0=`r mean(samples[,1])`, beta1=`r mean(samples[,2])`, beta2=`r mean(samples[,3])`, sigma2=`r mean(samples[,4])`. 

The lenght of the burn-in period was chosen looking at the trace plots.
Given the high correlation structure of the sample, we do not expect the estimation to be reliable, especially for $\sigma^2$.
To improve the accuracy, some correlation reduction technique should be employed, such as thinning, leading to a substantial increase of the computation time.

As an alternative, we can note that

\begin{equation}
  f(\beta, \sigma^2) = f(\beta | \sigma^2) f(\sigma^2)
\end{equation}

With $\sigma^2 \sim \mathcal{IG}(a,b)$ and $\beta | \sigma^2$ is the posterior associated to the Zellner's g-prior, 
that is $\beta | \sigma^2 \sim \mathcal{N}_3(q\hat{B}+(1-q)B, q\sigma^2(X^TX)^{-1})$, where $q=g/(1+g)$ and $\hat{B}=(X^TX)^{-1}X^Ty$.
Thus we can directly sample from these two distributions.

```{r echo=TRUE}
q <- g / (1+g)
Bhat <- Y %*% t(X) %*% y
mean <- q*Bhat + (1-q)*B
joint_sample <- function(n){
  # sample sigma^2
  s2 <- rinvgamma(n, shape=a, rate=b)
  b <- matrix(0, nrow=n, ncol=3)
  # sample beta given the value of sigma^2
  for (i in 1:n){
    S <- q * s2[i] * Y
     b[i,] <- rmvnorm(1, mean, S)
  }
  # return
  return (cbind(b,s2))
}
```

Sample again
```{r echo=TRUE}
samples <- matrix(0, nrow=n, ncol=4)
samples <- joint_sample(n)
```

```{r echo=FALSE}
par(mfrow = c(1,4))
hist(samples[,1], main=NULL, xlab=expression(beta[0]))
hist(samples[,2], main=NULL, xlab=expression(beta[1]))
hist(samples[,3], main=NULL, xlab=expression(beta[2]))
hist(sqrt(samples[,4]), main=NULL, xlab = expression(sigma))
par(mfrow = c(1, 1))
```

Mean values: beta0=`r mean(samples[,1])`, beta1=`r mean(samples[,2])`, beta2=`r mean(samples[,3])`, sigma2=`r mean(samples[,4])`.

### Use the function \texttt{blinreg} in package `LearnBayes` to redo the simulation and compare the results.

```{r echo=TRUE}
prior <- list(b0=B, c0=g)
LBsimul <- blinreg(y, X, n, prior)
beta_simul <- LBsimul$beta
sigma_simul <- sqrt(LBsimul$sigma)
```

```{r echo=FALSE}
par(mfrow = c(1,4))
hist(beta_simul[,1], main=NULL, xlab=expression(beta[0]))
hist(beta_simul[,2], main=NULL, xlab=expression(beta[1]))
hist(beta_simul[,3], main=NULL, xlab=expression(beta[2]))
hist(sigma_simul, main=NULL, xlab = expression(sigma))
par(mfrow = c(1, 1))
```

Mean values: beta0=`r mean(beta_simul[,1])`, beta1=`r mean(beta_simul[,2])`, beta2=`r mean(beta_simul[,3])`, sigma2=`r mean(sigma_simul)`.

As expected, looking qualitatively at the histograms, it is possible to see how more consistent results are obtained for the second sample,
at least concerning the mean value. At the same time, a smaller variance is obtained.

### From the simulated samples, compute the posterior means and standard deviations of $\beta_1$ and $\beta_2$.

```{r echo=TRUE}
beta1 <- mean(samples[,2])
dbeta1 <- sd(samples[,2])
beta2 <- mean(samples[,3])
dbeta2 <- sd(samples[,3])

beta1_simul <- mean(beta_simul[,2])
dbeta1_simul <- sd(beta_simul[,2])
beta2_simul <- mean(beta_simul[,3])
dbeta2_simul <- sd(beta_simul[,3])
```

Second sample

| Parameter  | Mean value         | Standard deviation     |
|------------|--------------------|------------------------|
| $\beta_1$  | `r round(beta1,4)` | `r round(dbeta1,4)`    |
| $\beta_2$  | `r round(beta2,4)` | `r round(dbeta2,4)`    |


Using the `blinreg` function

| Parameter  | Mean value               | Standard deviation           |
|------------|--------------------------|------------------------------|
| $\beta_1$  | `r round(beta1_simul,4)` | `r round(dbeta1_simul,4)`    |
| $\beta_2$  | `r round(beta2_simul,4)` | `r round(dbeta2_simul,4)`    |



### Check the consistency of the posterior means and standard deviations with the least-squares estimates and associated standard errors from the `lm` run.

As anticipated, the mean values calculated with the MH algorithm, with an exception for $\sigma^2$, are quite similar to the results obtained with the `blinreg` function.
The deviations are due to the correlation structure of the sample.

The sample obtained from $f(\sigma^2)$ and $f(\beta | \sigma^2)$ gives very similar mean values, but with a smaller variance.

Finally, the sample obtained from the `blinreg` function is the only one for which not only the mean values, but also the variance is compatible with the esitmations given by the least-squares fit. 
This is not a surprise, since the `blinreg` function utilizes `lm` within it. 


### Suppose one is interested in estimating the expected `birthweight` for `male` and `female` babies of gestational weeks `36` and `40`. From the simulated draws of the posterior distribution construct 90\% interval estimates for 36-week males, 36-week females, 40-week males, and 40-week females.

```{r echo=TRUE}
beta0 <- mean(samples[,1])
CI_mean <- beta0 + beta1*c(36,36,40,40) + beta2*c(0,1,0,1)
CI_inf <- qnorm(0.05, mean=CI_mean, sd=c(dbeta1, dbeta2))
CI_sup <- qnorm(0.95, mean=CI_mean, sd=c(dbeta1, dbeta2))
CI <- cbind(CI_inf, CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```


### Compare the results you obained with those you can have from function `blinregexpected`.

```{r echo=TRUE}
X1 <- rbind(c(36,0),c(36,1),c(40,0),c(40,1))
X1 <- cbind(1, X1)
exp_response <- blinregexpected(X1, LBsimul)
# Confidence interval construction
exp_mean <- apply(exp_response, MARGIN=2, mean)
exp_sd <- apply(exp_response, MARGIN=2, sd) 
CI_inf <- qnorm(0.05, mean=exp_mean, sd=exp_sd)
CI_sup <- qnorm(0.95, mean=exp_mean, sd=exp_sd)
CI <- cbind(CI_inf, CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```

Consistent with the fact of having used a sample with larger variance (the `blinreg` sample). Write this better, or even better use the same sample in both points. 

### Suppose instead that one wishes to predict the birthweight for a 36- week male, a 36-week female, a 40-week male, and a 40-week female. Use the simulated data  from the posterior to construct 90\% prediction intervals for the birthweight for each type of baby.

```{r echo=TRUE}
pred_sample <- samples[,1:3]%*%t(X1) + rnorm(n)*samples[,4]
CI_mean<- apply(pred_sample, MARGIN=2, mean)
CI_sd <- apply(pred_sample, MARGIN=2, sd)
CI_inf <- qnorm(0.05, mean=CI_mean, sd=CI_sd)
CI_sup <- qnorm(0.95, mean=CI_mean, sd=CI_sd)
CI <- cbind(CI_inf,CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```

### Compare the results you obained with those you can have from function `blinregpred`

```{r echo=TRUE}
pred_response <- blinregpred(X1, LBsimul)
# Confidence interval construction
pred_mean <- apply(pred_response, MARGIN=2, mean)
pred_sd <- apply(pred_response, MARGIN=2, sd) 
CI_inf <- qnorm(0.05, mean=pred_mean, sd=pred_sd)
CI_sup <- qnorm(0.95, mean=pred_mean, sd=pred_sd)
CI <- cbind(CI_inf, CI_sup)
colnames(CI) <- c("CI_inf", "CI_sup")
rownames(CI) <- c("36-Female","36-Male","40-Female","40-Male")
print(CI)
```

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 3: Athlete
    
For a given professional athlete, his or her performance level will tend to increase until midcareer and then deteriorate until retirement. 
Let $y_i$ denote the number of home runs hit by the professional baseball player Mike Schmidt in $n_i$ at-bats (opportunities) during the ith season. 
The datafile `schmidt` in `LearnBayes` package gives Schmidt's age, $y_i$, and $n_i$ for all 18 years of his baseball career. 
If $y_i$ is assumed to be $binomial(n_i, p_i)$, where $p_i$ denotes the probability of hitting a home run during the ith season, 
then a reasonable model for the ${p_i}$ is the logit quadratic model of the form:

$$
    \log \left(\frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 AGE_i + \beta_2 AGE^2_i
$$

where $AGE_i$ is Schmidt's age during the ith session. 
Assume that the regression vector $\boldsymbol{\beta} = (\beta_0,\beta_1, \beta_2)^\top$ has a uniform non-informative prior.
  
### Write a short R function to compute the logarithm of the posterior density of $\beta$.

```{r echo=TRUE}
data <- LearnBayes::schmidt
y <- data$HR
n <- data$AB
age <- data$Age
```

Let $N=18$ and $g_i(\beta) = \beta_0 + \beta_1 AGE_i + \beta_2 AGE_i^2$. The posterior density for $\beta$ is given by, using Bayes's theorem:

\begin{align}
  f(\beta | \textbf{y}) &\propto f(\textbf{y} | \beta) f(\beta) \propto f(\textbf{y} | \beta) \\
  &\propto \prod_{i=1}^{N} f(y_i | \beta) = \prod_{i=1}^{N} \binom{n_i}{y_i} p_i^{y_i} (1-p_i)^{n_i-y_i}  
\end{align}

where the dependence on $\beta$ is in $p_i = p_i(\beta)$. Exploiting this dependence:

\begin{align}
  f(\beta | \textbf{y}) &\propto \prod_{i=1}^{N} \binom{n_i}{y_i} \left(\frac{p_i}{1-p_i}\right)^{y_i} (1-p_i)^{n_i} \\
  &= \prod_{i=1}^{N} \binom{n_i}{y_i} e^{g_i(\beta)y_i} \left( \frac{1}{1+e^{g_i(\beta)}} \right)^{n_i}
\end{align}

Therefore the log-posterior density is given, up to an additive constant, by 

\begin{equation}
  \log{f(\beta | \textbf{y})} = \sum_{i=1}^{N} \left[ \log{\binom{n_i}{y_i}} + g_i(\beta) y_i - n_i \log{(1+e^{g_i(\beta)})} \right]
\end{equation}

```{r echo=TRUE}
g_func <- function(beta){
  stopifnot(length(beta) == 3)
  res <- beta[1] + beta[2]*age + beta[3]*age^2
  return (res)
}
logposterior <- function(beta){
  stopifnot(length(beta) == 3)
  g <- g_func(beta)
  res <- lchoose(n,y) + g*y - n*log(1+exp(g))
  res <- sum(res)
  return (res)
}
```


### Find the laplace approximation of the posterior for $\boldsymbol{\beta}$, build an R function to compute it. 

```{r echo=TRUE}
start   <- c(0,0,0)
control <- list(fnscale=-1, ndeps=rep(1e-6, length(start)))
laplace <- optim(par=start, fn=logposterior, control=control, hessian=TRUE)
mode    <- laplace$par
V       <- -solve(laplace$hessian) 
```

```{r echo=TRUE}
# Laplace's approximation
logproposal <- function(x, m){
  return (dmvnorm(x, m, V, log=TRUE))
}
```

### Build a Metropolis-Hasting using the laplace approximation as proposal to simulate 5000 draws from the posterior distribution of $\boldsymbol{\beta}$.

```{r echo=TRUE}
MH <- function(x_start, logtarget, n){
  samples <- matrix(0, nrow=n+1, ncol=3)
  samples[1,] <- x_start
  x_curr <- x_start
  # iteration
  for (i in 1:n){
    # sample from proposal
    x_prop <- mvrnorm(n=1, mu=x_curr, Sigma=V)
    # calculate acceptance probability
    acceptance_ratio <- logtarget(x_prop) - logtarget(x_curr)
    # accept or reject the proposal step
    if (log(runif(1)) < acceptance_ratio){
      x_curr <- x_prop
    }
    samples[i+1,] <- x_curr
  }
  # return samples
  samples <- samples[-1,]
  return (samples)
}
```

Sampling
```{r echo=TRUE}
n_samples <- 5000
x_start <- mode
samples <- MH(x_start, logposterior, n_samples)
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 3))
plot(1:n_samples,samples[,1], type="l", xlab = expression(i), ylab = expression(beta[1]))
plot(1:n_samples,samples[,2], type="l", xlab = expression(i), ylab = expression(beta[2]))
plot(1:n_samples,samples[,3], type="l", xlab = expression(i), ylab = expression(beta[3]))
# Restore default plot options
par(mfrow = c(1, 1))
```

Correlation analysis
```{r echo=TRUE}
acf(samples)
```

```{r echo=FALSE}
# trace  to check convergence
par(mfrow = c(1, 3))
hist(samples[,1], main=NULL, xlab=expression(beta[0]))
hist(samples[,2], main=NULL, xlab=expression(beta[1]))
hist(samples[,3], main=NULL, xlab=expression(beta[2]))
# Restore default plot options
par(mfrow = c(1, 1))
```

### One would expect the fitted parabola to have a concave down shape where $\beta_2 < 0$. Use the simulation output from the previous point to find the posterior probability that the fitted curve is concave down.

```{r echo=TRUE}
beta2 <- samples[,3]
prob <- length(beta2[beta2[]<0]) / length(beta2)
```

```{r echo=FALSE}
print(paste0("Probability that", expression(beta[2]<0), ": ", round(prob,4), ")"))
```

```{r echo=FALSE}
# Clear workspace
rm(list=ls())
```

<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# Exercise 4 (Hepatitis): a normal hierarchical model with measurement error
  
This example is taken from Spiegelhalter et al (1996) (chapter in Markov Chain Monte Carlo in Practice) and concerns $N=106$ children whose 
post-vaccination anti Hb titre was measured $2$ or $3$ times. 
Both measurements and times have been transformed to a log scale. One covariate $y_0$= log titre at baseline, is available. 
The model is essentially a random effects linear growth curve

$$  
\begin{align*}
Y_{ij} & \sim Normal( \alpha_i + \beta_i ( t_{ij} - \bar{t}) + \gamma (y_{0i} - \bar{y}_0), \tau ) \\
\alpha_i & \sim Normal( \alpha_0 , \tau_\alpha ) \\
\beta_i & \sim Normal( \beta_0 , \tau_\beta ) \\
\end{align*}
$$

where, $\bar{t}$ is the time mean among the observations for which the value of $Y_{ij}$ is available, $\bar{y}_0 = \sum_{i=1}^N y_{0i}/N$ and 
$\tau$ represents the precision (1/variance) of a normal distribution. 
We note the absence of a parameter representing correlation between $\alpha_i$ and $\beta_i$ unlike in Gelfand et al 1990. 
$\alpha_0$, $\tau_\alpha$, $\beta_0$, $\tau_\beta$, $\tau$ are given independent ``noninformative'' priors.



### Run the chain using the following initial values: $\alpha_0=4$, $\beta_0=0$, $\gamma=0$, $\tau_\alpha=1$, $\tau_\beta=1$ and $\tau=1$. Run a burn in of $1000$ updates followed by a futher $10000$ updates.

I assume $\gamma = 1$ instead

```{r echo=TRUE}  
# Load data
load("hepatitis.Rdata")
data_list <- list(
  "N" = hepatitis$N,
  "T" = hepatitis$T,
  "y" = hepatitis$Y,
  "t" = hepatitis$t,
  "y0" = hepatitis$y0,
  "y0_bar" = sum(hepatitis$y0)/hepatitis$N,
  "t_bar" = mean(hepatitis$t),
  "gam" = 1
)
str(data_list)
```

```{r echo=TRUE}
library(rjags)
library(R2jags)

# Define the model
hierarchical <- "model {
  # Prior distributions
  alpha_0 ~ dnorm(0, 1e-4)
  beta_0 ~ dnorm(0, 1e-4)
  tau_alpha ~ dgamma(1e-2, 1e-2)
  tau_beta ~ dgamma(1e-2, 1e-2)
  tau ~ dgamma(1e-2, 1e-2)

  # Likelihood
  for (i in 1:N) {
     alpha[i] ~ dnorm(alpha_0, tau_alpha)
     beta[i] ~ dnorm(beta_0, tau_beta)
     for (j in 1:T) {
        y[i,j] ~ dnorm(alpha[i] + beta[i]*(t[i,j] - t_bar) + gam*(y0[i]-y0_bar), tau)
     }
  }
}"

# Specify desired parameters for inference
parameters <- c("alpha", "beta", "tau")

# Initial values for the chain
inits <- function() {
  list(
    alpha_0 = 4,
    beta_0 = 0,
    tau_alpha = 1,
    tau_beta = 1,
    tau = 1
  )
}

# Run
n <- 10000
burn.in <- 1000
n.thin <- 1

MCMCrun <- jags(
  data = data_list,
  inits = inits,
  parameters.to.save = parameters,
  model.file = textConnection(hierarchical),
  n.burnin = burn.in,
  n.iter = n.thin*n + burn.in,
  n.chains = 1,
  n.thin = n.thin,
  progress.bar = "none",
  quiet = TRUE,
  DIC = FALSE
)
```

Choice of the prior parameters... (add reference to book)

```{r echo=TRUE}
# Posterior samples extraction
samples_list <- MCMCrun$BUGSoutput$sims.list
str(samples_list)
# Get the number of parameters and samples
num_params <- ncol(samples_list[[1]])
num_samples <- nrow(samples_list[[1]])
# Convert list to matrix
samples <- cbind(samples_list$tau, samples_list$alpha, samples_list$beta)
names <- c("tau")
for (i in 1:num_params) {
   names <- c(names, paste0("alpha[", i, "]"))
}
for (i in 1:num_params) {
   names <- c(names, paste0("beta[", i, "]"))
}
colnames(samples) <- names
# Define the maximum number of rows and columns to display
max_row <- 4
max_cols <- 4
# Display a subset of rows and columns from the large matrix
subset_matrix <- samples[1:max_row, c(1, 2, 3, num_params+2, num_params+3)]
subset_matrix <- round(subset_matrix, 4)
subset_matrix
```

```{r echo=TRUE}
# Summary
summ <- MCMCrun$BUGSoutput$summary
# Display a subset of rows and columns from the large matrix
subset_matrix <- summ[c(nrow(summ), 1, 2, num_params+1, num_params+2),]
subset_matrix <- round(subset_matrix, 4)
subset_matrix
```


Remove plots

```{r echo=FALSE, eval=FALSE}
# trace  to check convergence ## DON'T RUN, VARIABLE NAMES HAS BEEN CHANGED
par(mfrow = c(1, 2))
plot(1:n,samples[,1], type="l", main=NULL, xlab = expression(i), ylab = expression(alpha[0]^(i)))
plot(1:n,samples[,2], type="l", main=NULL, xlab = expression(i), ylab = expression(beta[0]^(i)))
# Restore default plot options
par(mfrow = c(1, 1))
# correlation
acf(samples)
```


### Construct a posterior confidence intervals of size $0.95$ for $\alpha_i$ and $\beta_i$ parameters.

```{r echo=TRUE}
CI_lower <- summ[,"2.5%"]
CI_upper <- summ[,"97.5%"]
CI <- cbind(CI_lower, CI_upper)
# Print only CI for i=1
round(CI[c(1,num_params+1),], 4)
```


### Sample $10000$ values from the posterior predictive distribution of $Y$ for $t=(6,7,8)^\top$ and $y_0=6.5$.

```{r echo=TRUE}
t <- c(6,7,8)
y0 <- 6.5

alpha <- mean(samples_list$alpha)
beta <- mean(samples_list$beta)
tau <- mean(samples_list$tau) 

mu <- alpha + beta*(t-mean(t)) + data_list$gam * (y0 - data_list$y0_bar)
y_sample <- sapply(mu, function(m) rnorm(num_samples, m, tau))

y_mean <- apply(y_sample, 2, mean)
y_sd <- apply(y_sample, 2, sd)
```

```{r echo=FALSE}
print(paste0("Mean values: ", paste(round(y_mean, 4), collapse=", ")))
print(paste0("Standar deviations: ", paste(round(y_sd, 4), collapse=", "))) 
par(mfrow = c(1,3))
hist(y_sample[,1], main=NULL, xlab=expression(Y[1]))
hist(y_sample[,2], main=NULL, xlab=expression(Y[2]))
hist(y_sample[,3], main=NULL, xlab=expression(Y[3]))
par(mfrow = c(1, 1))
```


### Construct posterior predictive intervals for $Y$ of size $0.95$ for the previous case.

```{r echo=TRUE}
CI_lower <- round(y_mean+qnorm(0.025)*y_sd,4)
CI_upper <- round(y_mean+qnorm(0.975)*y_sd,4)
CI <- cbind(CI_lower, CI_upper)
names <- c()
for (i in 1:nrow(CI)) {
  names <- c(names, paste0("Y[", i, "]"))
}
rownames(CI) <- names
# Print only CI for i=1
round(CI, 4)
```

## In the previous setting we assumed that the baseline $y_{0i}$ was measured without errors. Construct a model where,

$$  
\begin{align*}
y_{0i} & \sim Normal(\mu_{0i}, \tau) \\
\mu_{0i} & \sim Normal(\theta, \psi)
\end{align*}
$$

and a ``non informative'' prior for $\theta$ and $\psi$.

### Redo the previous points with the new model, using initial values $\theta = 6$ and $\psi = 1$.

```{r echo=TRUE}  
# Load data
data_list <- list(
  "N" = hepatitis$N,
  "T" = hepatitis$T,
  "y" = hepatitis$Y,
  "t" = hepatitis$t,
  "t_bar" = mean(hepatitis$t),
  "gam" = 1
)
str(data_list)
```

```{r echo=TRUE}
# Define the model
hierarchical <- "model {
  # Prior distributions
  alpha_0 ~ dnorm(0, 1e-4)
  beta_0 ~ dnorm(0, 1e-4)
  tau_alpha ~ dgamma(1e-2, 1e-2)
  tau_beta ~ dgamma(1e-2, 1e-2)
  tau ~ dgamma(1e-2, 1e-2)
  theta ~ dnorm(0, 1e-4)
  psi ~ dgamma(1e-2, 1e-2)

  # Likelihood
  y0_bar = mean(y0)
  for (i in 1:N) {
     alpha[i] ~ dnorm(alpha_0, tau_alpha)
     beta[i] ~ dnorm(beta_0, tau_beta)
     mu0[i] ~ dnorm(theta, psi)
     y0[i] ~ dnorm(mu0[i], tau)
     for (j in 1:T) {
        y[i,j] ~ dnorm(alpha[i] + beta[i]*(t[i,j] - t_bar) + gam*(y0[i]-y0_bar), tau)
     }
  }
}"

# Specify desired parameters for inference
parameters <- c("alpha", "beta", "y0", "tau")

# Initial values for the chain
inits <- function() {
  list(
    alpha_0 = 4,
    beta_0 = 0,
    tau_alpha = 1,
    tau_beta = 1,
    tau = 1,
    theta = 6,
    psi = 1,
    y0 = rep(0,hepatitis$N)
  )
}

# Run
MCMCrun <- jags(
  data = data_list,
  inits = inits,
  parameters.to.save = parameters,
  model.file = textConnection(hierarchical),
  n.burnin = burn.in,
  n.iter = n.thin*n + burn.in,
  n.chains = 1,
  n.thin = n.thin,
  progress.bar = "none",
  quiet = TRUE,
  DIC = FALSE
)
```

```{r echo=TRUE}
# Posterior samples extraction
samples_list <- MCMCrun$BUGSoutput$sims.list
str(samples_list)
# Get the number of parameters and samples
num_params <- ncol(samples_list[[1]])
num_samples <- nrow(samples_list[[1]])
# Convert list to matrix
samples <- cbind(samples_list$tau, samples_list$y0, samples_list$alpha, samples_list$beta)
names <- c("tau")
for (i in 1:num_params) {
   names <- c(names, paste0("y0[", i, "]"))
}
for (i in 1:num_params) {
   names <- c(names, paste0("alpha[", i, "]"))
}
for (i in 1:num_params) {
   names <- c(names, paste0("beta[", i, "]"))
}
colnames(samples) <- names
# Define the maximum number of rows and columns to display
max_row <- 4
max_cols <- 4
# Display a subset of rows and columns from the large matrix
subset_matrix <- samples[1:max_row, c(1, 2, num_params+2, 2*num_params+2)]
subset_matrix <- round(subset_matrix, 4)
subset_matrix
```

```{r echo=TRUE}
# Summary
summ <- MCMCrun$BUGSoutput$summary
# Display a subset of rows and columns from the large matrix
subset_matrix <- summ[c(2*num_params+1, 1,num_params+1, 2*num_params+2),]
subset_matrix <- round(subset_matrix, 4)
subset_matrix
```

- Confidence interval

```{r echo=TRUE}
CI_lower <- summ[,"2.5%"]
CI_upper <- summ[,"97.5%"]
CI <- cbind(CI_lower, CI_upper)
# Print only CI for i=1
round(CI[c(1,num_params+1),], 4)
```

- Sample $10000$ values from the posterior predictive distribution of $Y$ for $t=(6,7,8)^\top$ and $y_0=6.5$. : modificare da qui

```{r echo=TRUE}
# t <- c(6,7,8)

# y0 <- colMeans(samples_list$y0)
# y0_bar <- mean(y0)

# alpha <- mean(samples_list$alpha)
# beta <- mean(samples_list$beta)
# tau <- mean(samples_list$tau) 

# mu <- alpha + beta*(t-mean(t)) + data_list$gam * (y0 - y0_bar)
# y_sample <- sapply(mu, function(m) rnorm(num_samples, m, tau))

# y_mean <- apply(y_sample, 2, mean)
# y_sd <- apply(y_sample, 2, sd)
```

```{r echo=FALSE}
# print(paste0("Mean values: ", paste(round(y_mean, 4), collapse=", ")))
# print(paste0("Standar deviations: ", paste(round(y_sd, 4), collapse=", "))) 
# par(mfrow = c(1,3))
# hist(y_sample[,1], main=NULL, xlab=expression(Y[1]))
# hist(y_sample[,2], main=NULL, xlab=expression(Y[2]))
# hist(y_sample[,3], main=NULL, xlab=expression(Y[3]))
# par(mfrow = c(1, 1))
```


- Construct posterior predictive intervals for $Y$ of size $0.95$ for the previous case.

```{r echo=TRUE}
# CI_lower <- round(y_mean+qnorm(0.025)*y_sd,4)
# CI_upper <- round(y_mean+qnorm(0.975)*y_sd,4)
# CI <- cbind(CI_lower, CI_upper)
# names <- c()
# for (i in 1:nrow(CI)) {
#   names <- c(names, paste0("Y[", i, "]"))
# }
# rownames(CI) <- names
# # Print only CI for i=1
# round(CI, 4)
```


<!--------------------------------------------------------------------------------------------------------------------------------------------------->

# References

<!--------------------------------------------------------------------------------------------------------------------------------------------------->